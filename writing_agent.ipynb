{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c12ca2a",
   "metadata": {},
   "source": [
    "## what are we trying to do\n",
    "a graph with 2 agents. one that writes an article plan from a raw interview with an expert \n",
    "another agent that writes the blogpost from the plan \n",
    "## next steps (later)\n",
    "try to add interrupt\n",
    "and human feedback \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a394f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15a2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfff4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"meetup_writing_agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfb18f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    who: str = Field(\n",
    "        description=\"target reader of the blog post, e.g., 'AI researchers', 'business leaders', etc.\",\n",
    "    )\n",
    "    why: str = Field(\n",
    "        description=\"why are we writing this blog post.\",\n",
    "    )\n",
    "    what: str = Field(\n",
    "        description=\"what are the main topics to cover in the blog post.\",\n",
    "    )\n",
    "    the_issue: str = Field(\n",
    "        description=\"the main issue or problem that the blog post addresses.\",\n",
    "    )\n",
    "    where_we_stand: str = Field(\n",
    "        description=\"current position or perspective on the issue.\",\n",
    "    )\n",
    "    single_message: str = Field(\n",
    "        description=\"the single most important message to convey in the blog post.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def plan(self) -> str:\n",
    "        return (\n",
    "            f\"Who: {self.who}\\n\"\n",
    "            f\"Why: {self.why}\\n\"\n",
    "            f\"What: {self.what}\\n\"\n",
    "            f\"Issue: {self.the_issue}\\n\"\n",
    "            f\"Where We Stand: {self.where_we_stand}\\n\"\n",
    "            f\"Single Message: {self.single_message}\\n\"\n",
    "        )\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    plan: List[Plan] = Field(\n",
    "        description=\"AI generated plan / scaffold for the blog post.\",\n",
    "    )\n",
    "\n",
    "class BlogPost(BaseModel):\n",
    "    title: str = Field(description=\"The title of the blog post.\")\n",
    "    content: str = Field(description=\"The full content of the blog post.\")\n",
    "\n",
    "    \n",
    "class GenerateBlogState(TypedDict):\n",
    "    company_strategy: str  # Company strategy\n",
    "    content_strategy: str      # content strategy\n",
    "    plan: List[Plan]       # AI generated plan / scaffold for the blog post.\n",
    "    human_analyst_feedback: str # Human feedback\n",
    "    blog_post: List[BlogPost]  # the substance of the blog posts, with a {\"title\": str, \"content\": str} written according to the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fdfa526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "plan_instructions = \"\"\"You are tasked with creating a plan for a proffesional blog post for a company called Big Kids Automation Agency. the plan is just a skeleton of a blog post which contains\n",
    "some question and answers that will help the company writer to write a focused blog post. this plan is a set of instruction to the writer to stay focus and write the post. \n",
    " \n",
    "The raw_input is a transcript of a conversation with an expert.  i want you to use this input as the basis for the plan of the blog post. you take this\n",
    "as the source where you will extract the ideas for the blog post.\n",
    "\n",
    "Follow these instructions carefully:\n",
    "\n",
    "1. First, review the company business strategy. Examine the company strategy:\n",
    "{company_strategy}\n",
    "        \n",
    "2. The blog post must fit the company content strategy. Examine the company content strategy: \n",
    "        \n",
    "{content_strategy}\n",
    "\n",
    "3. Read the raw_imput.\n",
    "    {raw_input}\n",
    "4. Create a plan for a blog post. The plan should include the following sections:\n",
    "\n",
    "who , why, what, the_issue, where_we_stand, single_message.\n",
    "\n",
    "5. Examine any editoral feedback from the human analyst. If there is any feedback, incorporate it into the plan. \n",
    "{human_analyst_feedback}\n",
    "\n",
    "\n",
    "                    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b6252c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "raw_input_path = Path(\"data/processed/raw_input.mkd\")\n",
    "company_strategy_path = Path(\"data/processed/company_strategy.mkd\")\n",
    "content_strategy_path = Path(\"data/processed/content_strategy.mkd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b0298de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plan(state: GenerateBlogState):\n",
    "    \"\"\"Create plan\"\"\"\n",
    "    company_strategy = state['company_strategy']\n",
    "    content_strategy = state['content_strategy']\n",
    "    human_analyst_feedback=state.get('human_analyst_feedback', '')\n",
    "\n",
    "    \n",
    "    # Read the company strategy\n",
    "    try:\n",
    "        with open(\"company_strategy_path\", \"r\", encoding=\"utf-8\") as f:\n",
    "            company_strategy = f.read()\n",
    "    except FileNotFoundError:\n",
    "        raw_input = \"No raw input file found.\"\n",
    "    \n",
    "    # Read the content strategy\n",
    "    try:\n",
    "        with open(\"content_strategy_path\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content_strategy = f.read()\n",
    "    except FileNotFoundError:\n",
    "        raw_input = \"No raw input file found.\"\n",
    "    \n",
    "    # Read raw_input file\n",
    "    try:\n",
    "        with open(\"raw_input_path\", \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_input = f.read()\n",
    "    except FileNotFoundError:\n",
    "        raw_input = \"No raw input file found.\"\n",
    "\n",
    "    \n",
    "\n",
    "    # Enforce structured output\n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    # System message\n",
    "    system_message = plan_instructions.format(\n",
    "        company_strategy=company_strategy,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        content_strategy=content_strategy,\n",
    "        raw_input=raw_input\n",
    "    )\n",
    "\n",
    "    # Generate question \n",
    "    plan = structured_llm.invoke([SystemMessage(content=system_message)] + [HumanMessage(content=\"Generate the article plan.\")])\n",
    "    \n",
    "    # Write the plan to the state\n",
    "    return {\"plan\": plan.plan}\n",
    "\n",
    "def human_analyst_feedback(state: GenerateBlogState):\n",
    "    \"\"\" No-op node that should be interrupted on \"\"\"\n",
    "    pass\n",
    "\n",
    "def should_continue(state: GenerateBlogState):\n",
    "    \"\"\" Return the next node to execute \"\"\"\n",
    "\n",
    "    # Check if human feedback\n",
    "    human_analyst_feedback=state.get('human_analyst_feedback', None)\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_plan\"\n",
    "    \n",
    "    # Otherwise go to the next node\n",
    "    return \"write_blog_post\"\n",
    "   \n",
    "def write_blog_post(state: GenerateBlogState):\n",
    "    \"\"\"Generate a blog post draft from the plan.\"\"\"\n",
    "    \n",
    "    blog_post_instructions = (\n",
    "        \"You are a professional blog writer for Big Kids Automation Agency. \"\n",
    "        \"Using the following plan, write a complete blog post. \"\n",
    "        f'Plan: {state[\"plan\"]}\\n'\n",
    "\n",
    "        \"Make sure the post is engaging, clear, and follows the structure and intent of the plan. \"\n",
    "        \"Return a title and the full content of the blog post.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    structured_llm = llm.with_structured_output(BlogPost)\n",
    "    blog_post = structured_llm.invoke([SystemMessage(content=blog_post_instructions), HumanMessage(content=\"Write the blog post.\")])\n",
    "    # Return the blog_post as part of the state\n",
    "    return {\"blog_post\": [blog_post]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47e25588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes and edges \n",
    "builder = StateGraph(GenerateBlogState)\n",
    "builder.add_node(\"create_plan\", create_plan)\n",
    "builder.add_node(\"human_analyst_feedback\", human_analyst_feedback)\n",
    "builder.add_node(\"write_blog_post\", write_blog_post)\n",
    "\n",
    "builder.add_edge(START, \"create_plan\")\n",
    "builder.add_edge(\"create_plan\", \"human_analyst_feedback\")\n",
    "builder.add_conditional_edges(\"human_analyst_feedback\", should_continue, [\"create_plan\", \"write_blog_post\"])\n",
    "builder.add_edge(\"write_blog_post\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=['human_analyst_feedback'], checkpointer=memory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54389ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PLAN GENERATED ===\n",
      "Who: SME owners and managers\n",
      "Why: To educate SME owners and managers on the transformative potential of AI and automation in improving business efficiency and employee satisfaction.\n",
      "What: The blog post will cover the benefits of AI and automation for SMEs, how these technologies can save time and money, enhance employee happiness, and improve tech understanding. It will also discuss the agency's approach to automation and the importance of a healthier interdependence with tech tools.\n",
      "Issue: Many SMEs struggle with inefficient processes and a lack of understanding of how to leverage AI and automation to improve their operations.\n",
      "Where We Stand: Big Kids Automation Agency believes in promoting a healthier interdependence with tech tools, freeing up time for SMEs through process automation, and liberating time through AI for SMEs.\n",
      "Single Message: AI and automation can transform SMEs by saving time, reducing costs, and enhancing employee satisfaction, leading to a healthier relationship with technology.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "thread = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "\n",
    "    for event in graph.stream({\"company_strategy\": company_strategy, \"content_strategy\": content_strategy}, thread, stream_mode=\"values\"):\n",
    "        # Review\n",
    "        plan = event.get('plan', '')\n",
    "        blog_post = event.get('blog_post', None)\n",
    "        \n",
    "        if plan:\n",
    "            print(\"=== PLAN GENERATED ===\")\n",
    "            for p in plan:\n",
    "                print(f\"Who: {p.who}\")\n",
    "                print(f\"Why: {p.why}\")\n",
    "                print(f\"What: {p.what}\")\n",
    "                print(f\"Issue: {p.the_issue}\")\n",
    "                print(f\"Where We Stand: {p.where_we_stand}\")\n",
    "                print(f\"Single Message: {p.single_message}\")\n",
    "                print(\"-\" * 50)\n",
    "        \n",
    "        if blog_post:\n",
    "            print(\"=== BLOG POST GENERATED ===\")\n",
    "            if isinstance(blog_post, list) and blog_post:\n",
    "                post = blog_post[0]\n",
    "                print(\"Title:\", post.title)\n",
    "                print(\"\\nContent:\\n\", post.content)\n",
    "            elif hasattr(blog_post, 'title'):\n",
    "                print(\"Title:\", blog_post.title)\n",
    "                print(\"\\nContent:\\n\", blog_post.content)\n",
    "            print(\"=== GENERATION COMPLETE ===\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error during graph execution: {e}\")\n",
    "    # Check the current state\n",
    "    state = graph.get_state(thread)\n",
    "    print(f\"Current state keys: {state.values.keys() if state.values else 'No state'}\")\n",
    "    print(f\"Next node: {state.next}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a009103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get state and look at next node\n",
    "state = graph.get_state(thread)\n",
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a97fb11",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'__end__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# We now update the state as if we are the human_feedback node\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman_feedback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplease include a reference to security issues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_node\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman_feedback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tech/ai_content_ops/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2328\u001b[39m, in \u001b[36mPregel.update_state\u001b[39m\u001b[34m(self, config, values, as_node, task_id)\u001b[39m\n\u001b[32m   2317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_state\u001b[39m(\n\u001b[32m   2318\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2319\u001b[39m     config: RunnableConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2322\u001b[39m     task_id: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2323\u001b[39m ) -> RunnableConfig:\n\u001b[32m   2324\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Update the state of the graph with the given values, as if they came from\u001b[39;00m\n\u001b[32m   2325\u001b[39m \u001b[33;03m    node `as_node`. If `as_node` is not provided, it will be set to the last node\u001b[39;00m\n\u001b[32m   2326\u001b[39m \u001b[33;03m    that updated the state, if not ambiguous.\u001b[39;00m\n\u001b[32m   2327\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbulk_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStateUpdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tech/ai_content_ops/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:1861\u001b[39m, in \u001b[36mPregel.bulk_update_state\u001b[39m\u001b[34m(self, config, supersteps)\u001b[39m\n\u001b[32m   1857\u001b[39m current_config = patch_configurable(\n\u001b[32m   1858\u001b[39m     config, {CONFIG_KEY_THREAD_ID: \u001b[38;5;28mstr\u001b[39m(config[CONF][CONFIG_KEY_THREAD_ID])}\n\u001b[32m   1859\u001b[39m )\n\u001b[32m   1860\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m superstep \u001b[38;5;129;01min\u001b[39;00m supersteps:\n\u001b[32m-> \u001b[39m\u001b[32m1861\u001b[39m     current_config = \u001b[43mperform_superstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuperstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m current_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tech/ai_content_ops/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:1796\u001b[39m, in \u001b[36mPregel.bulk_update_state.<locals>.perform_superstep\u001b[39m\u001b[34m(input_config, updates)\u001b[39m\n\u001b[32m   1794\u001b[39m     run = RunnableSequence(*writers) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(writers) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m writers[\u001b[32m0\u001b[39m]\n\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# execute task\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1796\u001b[39m     \u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1799\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1800\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUpdateState\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# deque.extend is thread-safe\u001b[39;49;00m\n\u001b[32m   1803\u001b[39m \u001b[43m                \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1804\u001b[39m \u001b[43m                \u001b[49m\u001b[43mCONFIG_KEY_TASK_ID\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1805\u001b[39m \u001b[43m                \u001b[49m\u001b[43mCONFIG_KEY_READ\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mlocal_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m_scratchpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[43m                        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmanaged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1819\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[38;5;66;03m# save task writes\u001b[39;00m\n\u001b[32m   1824\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task_id, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_task_ids, run_tasks):\n\u001b[32m   1825\u001b[39m     \u001b[38;5;66;03m# channel writes are saved to current checkpoint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tech/ai_content_ops/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3082\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3080\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3081\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3082\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tech/ai_content_ops/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tech/ai_content_ops/.venv/lib/python3.13/site-packages/langgraph/graph/_branch.py:169\u001b[39m, in \u001b[36mBranchSpec._route\u001b[39m\u001b[34m(self, input, config, reader, writer)\u001b[39m\n\u001b[32m    167\u001b[39m     value = \u001b[38;5;28minput\u001b[39m\n\u001b[32m    168\u001b[39m result = \u001b[38;5;28mself\u001b[39m.path.invoke(value, config)\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tech/ai_content_ops/.venv/lib/python3.13/site-packages/langgraph/graph/_branch.py:205\u001b[39m, in \u001b[36mBranchSpec._finish\u001b[39m\u001b[34m(self, writer, input, result, config)\u001b[39m\n\u001b[32m    202\u001b[39m     result = [result]\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ends:\n\u001b[32m    204\u001b[39m     destinations: Sequence[Send | \u001b[38;5;28mstr\u001b[39m] = [\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mends\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[32m    206\u001b[39m     ]\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    208\u001b[39m     destinations = cast(Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]], result)\n",
      "\u001b[31mKeyError\u001b[39m: '__end__'"
     ]
    }
   ],
   "source": [
    "# We now update the state as if we are the human_feedback node\n",
    "graph.update_state(thread, {\"human_feedback\": \n",
    "                            \"please include a reference to security issues\"}, as_node=\"human_feedback\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
