{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb001904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AssemblyAI API Key loaded: ✅\n",
      "Key starts with: 972365f41d...\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(project_root / '.env')\n",
    "\n",
    "# Test API key\n",
    "assemblyai_key = os.getenv('ASSEMBLYAI_API_KEY')\n",
    "print(f\"AssemblyAI API Key loaded: {'✅' if assemblyai_key else '❌'}\")\n",
    "print(f\"Key starts with: {assemblyai_key[:10] if assemblyai_key else 'None'}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc1fee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Tables in app.db:\n",
      "\n",
      "🔧 conversations:\n",
      "   - id (INTEGER)\n",
      "   - title (TEXT)\n",
      "   - raw_text (TEXT)\n",
      "   - source (TEXT)\n",
      "   - word_count (INTEGER)\n",
      "   - created_at (DATETIME)\n",
      "   - status (TEXT)\n",
      "\n",
      "🔧 sqlite_sequence:\n",
      "   - name ()\n",
      "   - seq ()\n",
      "\n",
      "🔧 blog_post_ideas:\n",
      "   - id (INTEGER)\n",
      "   - conversation_id (INTEGER)\n",
      "   - title (TEXT)\n",
      "   - description (TEXT)\n",
      "   - usefulness_potential (INTEGER)\n",
      "   - fitwith_seo_strategy (INTEGER)\n",
      "   - fitwith_content_strategy (INTEGER)\n",
      "   - inspiration_potential (INTEGER)\n",
      "   - collaboration_potential (INTEGER)\n",
      "   - innovation (INTEGER)\n",
      "   - difficulty (INTEGER)\n",
      "   - total_score (INTEGER)\n",
      "   - sent_to_prod (BOOLEAN)\n",
      "   - raw_llm_response (TEXT)\n",
      "   - created_at (DATETIME)\n",
      "\n",
      "🔧 processing_status:\n",
      "   - id (INTEGER)\n",
      "   - conversation_id (INTEGER)\n",
      "   - stage (TEXT)\n",
      "   - status (TEXT)\n",
      "   - error_message (TEXT)\n",
      "   - started_at (DATETIME)\n",
      "   - completed_at (DATETIME)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"data/app.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get all table names\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "print(\"📊 Tables in app.db:\")\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    \n",
    "    # Get column info for each table\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    columns = cursor.fetchall()\n",
    "    \n",
    "    print(f\"\\n🔧 {table_name}:\")\n",
    "    for col in columns:\n",
    "        print(f\"   - {col[1]} ({col[2]})\")  # column_name (type)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30f9e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Dependencies\n",
    "import assemblyai as aai\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict, Optional, List  # ← Added List here\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path  # ← Also added Path here\n",
    "\n",
    "# Import our database\n",
    "from database.db_operations import db\n",
    "from database.models import ConversationCreate\n",
    "\n",
    "print(\"✅ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e30147df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Pydantic Model for Structured Output\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class SpeakerRole(str, Enum):\n",
    "    \"\"\"Possible speaker roles in the conversation\"\"\"\n",
    "    CLIENT = \"client\"\n",
    "    INTERVIEWER = \"interviewer\"\n",
    "\n",
    "class Speaker(BaseModel):\n",
    "    \"\"\"Information about a person speaking in the conversation\"\"\"\n",
    "    name: Optional[str] = Field(default=None, description=\"Name of the speaker if mentioned\")\n",
    "    role: Optional[SpeakerRole] = Field(default=None, description=\"Role of the speaker in the conversation\")\n",
    "    company: Optional[str] = Field(default=None, description=\"Company they work for if mentioned\")\n",
    "\n",
    "class Challenge(BaseModel):\n",
    "    \"\"\"A challenge or problem mentioned in the conversation\"\"\"\n",
    "    description: Optional[str] = Field(default=None, description=\"Description of the challenge\")\n",
    "    impact: Optional[str] = Field(default=None, description=\"How this challenge affects them\")\n",
    "    urgency: Optional[str] = Field(default=None, description=\"Low, Medium, or High urgency\")\n",
    "\n",
    "class CurrentSolution(BaseModel):\n",
    "    \"\"\"How they currently solve their problems\"\"\"\n",
    "    solution: Optional[str] = Field(default=None, description=\"What they're currently doing\")\n",
    "    satisfaction_level: Optional[str] = Field(default=None, description=\"How satisfied they are: Very Satisfied, Satisfied, Neutral, Unsatisfied, Very Unsatisfied\")\n",
    "    limitations: Optional[List[str]] = Field(default=[], description=\"Limitations of current solution\")\n",
    "\n",
    "class Need(BaseModel):\n",
    "    \"\"\"A need identified using psychology frameworks like NVC\"\"\"\n",
    "    need_category: Optional[str] = Field(default=None, description=\"Category of need (e.g., autonomy, efficiency, security, connection)\")\n",
    "    description: Optional[str] = Field(default=None, description=\"Specific need description\")\n",
    "    intensity: Optional[str] = Field(default=None, description=\"Low, Medium, or High intensity\")\n",
    "\n",
    "class ExtractedInsights(BaseModel):\n",
    "    \"\"\"Complete structured output from conversation analysis\"\"\"\n",
    "    \n",
    "    # Speakers\n",
    "    speakers: Optional[List[Speaker]] = Field(default=[], description=\"People identified in the conversation\")\n",
    "    \n",
    "    # What they care about\n",
    "    core_values: Optional[List[str]] = Field(default=[], description=\"What this person/company cares about most\")\n",
    "    priorities: Optional[List[str]] = Field(default=[], description=\"Their current priorities and focus areas\")\n",
    "    \n",
    "    # Challenges\n",
    "    primary_challenges: Optional[List[Challenge]] = Field(default=[], description=\"Main problems they're facing\")\n",
    "    secondary_challenges: Optional[List[Challenge]] = Field(default=[], description=\"Secondary or related problems\")\n",
    "    \n",
    "    # Current solutions\n",
    "    current_solutions: Optional[List[CurrentSolution]] = Field(default=[], description=\"How they solve problems today\")\n",
    "    \n",
    "    # Needs analysis\n",
    "    psychological_needs: Optional[List[Need]] = Field(default=[], description=\"Underlying needs using NVC or similar frameworks\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19fad4ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 3: Define LangGraph State\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mAudioPipelineState\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mTypedDict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mAudioPipelineState\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m conversation_id: Optional[\u001b[38;5;28mint\u001b[39m]\n\u001b[32m      7\u001b[39m extracted_insights: Optional[ExtractedInsights]  \n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m raw_blog_ideas: Optional[List[\u001b[43mDict\u001b[49m]]        \u001b[38;5;66;03m# From creative agent\u001b[39;00m\n\u001b[32m      9\u001b[39m scored_blog_ideas: Optional[List[Dict]]     \u001b[38;5;66;03m# From analyst agent (future)\u001b[39;00m\n\u001b[32m     10\u001b[39m saved_idea_ids: Optional[List[\u001b[38;5;28mint\u001b[39m]]         \u001b[38;5;66;03m# From saver agent (future)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define LangGraph State\n",
    "class AudioPipelineState(TypedDict):\n",
    "    file_path: str\n",
    "    filename: str\n",
    "    transcript_text: Optional[str]\n",
    "    conversation_id: Optional[int]\n",
    "    extracted_insights: Optional[ExtractedInsights]  \n",
    "    raw_blog_ideas: Optional[List[Dict]]        # From creative agent\n",
    "    scored_blog_ideas: Optional[List[Dict]]     # From analyst agent (future)\n",
    "    saved_idea_ids: Optional[List[int]]         # From saver agent (future)\n",
    "    \n",
    "    # Status & error handling\n",
    "    status: str\n",
    "    error: Optional[str]\n",
    "\n",
    "\n",
    "print(\"✅ State defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850b086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded company strategy (6517 chars)\n",
      "✅ Loaded SEO strategy (5566 chars)\n",
      "📊 Strategy context keys: ['company_strategy', 'seo_strategy']\n"
     ]
    }
   ],
   "source": [
    "#ad Company Strategy Context\n",
    "def load_company_strategy_context():\n",
    "    \"\"\"Load company strategy and SEO strategy for creative context\"\"\"\n",
    "    \n",
    "    strategy_context = {}\n",
    "    \n",
    "    try:\n",
    "        # Load company strategy\n",
    "        company_strategy_path = \"../data/processed/company_strategy.mkd\"\n",
    "        with open(company_strategy_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            strategy_context[\"company_strategy\"] = f.read()\n",
    "        print(f\"✅ Loaded company strategy ({len(strategy_context['company_strategy'])} chars)\")\n",
    "        \n",
    "        # Load SEO strategy if exists\n",
    "        seo_strategy_path = \"../data/processed/seo_strategy.mkd\"\n",
    "        if os.path.exists(seo_strategy_path):\n",
    "            with open(seo_strategy_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                strategy_context[\"seo_strategy\"] = f.read()\n",
    "            print(f\"✅ Loaded SEO strategy ({len(strategy_context['seo_strategy'])} chars)\")\n",
    "        else:\n",
    "            strategy_context[\"seo_strategy\"] = \"No specific SEO strategy document found.\"\n",
    "            print(\"⚠️ No SEO strategy document found, using default guidance\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading strategy documents: {e}\")\n",
    "        strategy_context = {\n",
    "            \"company_strategy\": \"Strategy document not available\",\n",
    "            \"seo_strategy\": \"SEO strategy document not available\"\n",
    "        }\n",
    "    \n",
    "    return strategy_context\n",
    "\n",
    "# Test loading\n",
    "strategy_context = load_company_strategy_context()\n",
    "print(f\"📊 Strategy context keys: {list(strategy_context.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1b52ee1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell : Creative Agent Function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_blog_ideas_from_insights\u001b[39m(insights: ExtractedInsights, strategy_context: \u001b[38;5;28mdict\u001b[39m) -> List[\u001b[43mDict\u001b[49m]:\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    Creative agent that generates blog post ideas from extracted insights\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    rooted in Big Kids Automation's business context\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m     creative_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m    You are a creative content strategist for Big Kids Automation, a company that helps businesses implement AI and automation solutions.\u001b[39m\n\u001b[32m     10\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m \u001b[33m    Focus on being creative but grounded in real business value. These ideas should feel inspired by the conversation but applicable to a broader audience.\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell : Creative Agent Function\n",
    "def generate_blog_ideas_from_insights(insights: ExtractedInsights, strategy_context: dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Creative agent that generates blog post ideas from extracted insights\n",
    "    rooted in Big Kids Automation's business context\n",
    "    \"\"\"\n",
    "    \n",
    "    creative_prompt = f\"\"\"\n",
    "    You are a creative content strategist for Big Kids Automation, a company that helps businesses implement AI and automation solutions.\n",
    "    \n",
    "    COMPANY CONTEXT:\n",
    "    {strategy_context.get('company_strategy', 'Strategy not available')[:1000]}...\n",
    "    \n",
    "    SEO STRATEGY:\n",
    "    {strategy_context.get('seo_strategy', 'SEO strategy not available')[:500]}...\n",
    "    \n",
    "    CONVERSATION INSIGHTS TO WORK FROM:\n",
    "    \n",
    "    Speakers: {[f\"{s.name} ({s.role}) from {s.company}\" for s in insights.speakers] if insights.speakers else \"Unknown speakers\"}\n",
    "    \n",
    "    Core Values: {\", \".join(insights.core_values) if insights.core_values else \"None identified\"}\n",
    "    \n",
    "    Priorities: {\", \".join(insights.priorities) if insights.priorities else \"None identified\"}\n",
    "    \n",
    "    Primary Challenges:\n",
    "    {chr(10).join([f\"- {c.description} (Impact: {c.impact}, Urgency: {c.urgency})\" for c in insights.primary_challenges]) if insights.primary_challenges else \"None identified\"}\n",
    "    \n",
    "    Current Solutions:\n",
    "    {chr(10).join([f\"- {s.solution} (Satisfaction: {s.satisfaction_level})\" for s in insights.current_solutions]) if insights.current_solutions else \"None identified\"}\n",
    "    \n",
    "    Psychological Needs:\n",
    "    {chr(10).join([f\"- {n.description} ({n.need_category}, {n.intensity} intensity)\" for n in insights.psychological_needs]) if insights.psychological_needs else \"None identified\"}\n",
    "    \n",
    "    TASK:\n",
    "    Generate 4-5 creative blog post ideas that:\n",
    "    1. Address the challenges and needs identified in this conversation\n",
    "    2. Align with Big Kids Automation's mission to help businesses with AI/automation\n",
    "    3. Provide value to potential clients facing similar challenges\n",
    "    4. Support our SEO and content marketing strategy\n",
    "    5. Are actionable and practical, not just theoretical\n",
    "    \n",
    "    For each blog post idea, provide:\n",
    "    - title: Clear, engaging title that includes relevant keywords\n",
    "    - description: 2-3 sentence description of what the post will cover\n",
    "    - target_audience: Who this post is primarily for\n",
    "    - content_angle: The unique angle or approach this post takes\n",
    "    - business_value: How this post helps our business goals\n",
    "    \n",
    "    Return ONLY a JSON array in this exact format:\n",
    "    [\n",
    "        {{\n",
    "            \"title\": \"How AI Invoicing Systems Eliminate Payment Tracking Chaos\",\n",
    "            \"description\": \"A practical guide showing how modern AI-powered invoicing systems solve the common problem of tracking payments across multiple transactions. Includes real case studies and implementation steps.\",\n",
    "            \"target_audience\": \"CTOs and finance directors at growing companies\",\n",
    "            \"content_angle\": \"Problem-solution with real case studies\",\n",
    "            \"business_value\": \"Attracts prospects struggling with financial process automation\"\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "    \n",
    "    Focus on being creative but grounded in real business value. These ideas should feel inspired by the conversation but applicable to a broader audience.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Generate ideas using Claude\n",
    "        response = llm.invoke(creative_prompt)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        blog_ideas = json.loads(response.content)\n",
    "        \n",
    "        print(f\"✅ Creative agent generated {len(blog_ideas)} blog ideas\")\n",
    "        return blog_ideas\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ JSON parsing error in creative agent: {e}\")\n",
    "        print(f\"📝 Raw response: {response.content[:300]}...\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in creative agent: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"✅ Creative agent function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creative_agent_node(state: AudioPipelineState) -> AudioPipelineState:\n",
    "    \"\"\"\n",
    "    LangGraph node that runs the creative agent\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"🎨 Starting creative blog idea generation...\")\n",
    "        \n",
    "        # Check if we have insights\n",
    "        insights = state.get('extracted_insights')\n",
    "        if not insights:\n",
    "            return {\n",
    "                **state,\n",
    "                \"error\": \"No extracted insights available for blog idea generation\",\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "        \n",
    "        print(f\"📊 Working with insights: {len(insights.primary_challenges)} challenges, {len(insights.psychological_needs)} needs\")\n",
    "        \n",
    "        # Load strategy context\n",
    "        strategy_context = load_company_strategy_context()\n",
    "        \n",
    "        # Generate blog ideas\n",
    "        blog_ideas = generate_blog_ideas_from_insights(insights, strategy_context)\n",
    "        \n",
    "        if blog_ideas:\n",
    "            print(f\"🎉 Generated {len(blog_ideas)} blog ideas:\")\n",
    "            for i, idea in enumerate(blog_ideas, 1):\n",
    "                print(f\"   {i}. {idea.get('title', 'No title')}\")\n",
    "            \n",
    "            return {\n",
    "                **state,\n",
    "                \"raw_blog_ideas\": blog_ideas,\n",
    "                \"status\": \"ideas_generated\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                **state,\n",
    "                \"error\": \"Creative agent failed to generate blog ideas\",\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in creative agent node: {e}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"error\": f\"Creative agent error: {str(e)}\",\n",
    "            \"status\": \"error\"\n",
    "        }\n",
    "\n",
    "print(\"✅ Creative agent node ready for LangGraph integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_creative_agent():\n",
    "    \"\"\"Test the creative agent with sample insights\"\"\"\n",
    "    \n",
    "    # Get a conversation with insights from your database\n",
    "    conversations = db.get_all_conversations()\n",
    "    \n",
    "    # Find one that was processed with insights\n",
    "    test_conversation = None\n",
    "    for conv in conversations:\n",
    "        if conv.status == \"completed\" and \"insights_extracted\" not in conv.status:\n",
    "            # This might have insights in the pipeline state\n",
    "            test_conversation = conv\n",
    "            break\n",
    "    \n",
    "    if test_conversation:\n",
    "        print(f\"🧪 Testing with conversation: {test_conversation.title}\")\n",
    "        \n",
    "        # You'll need to extract insights first or use mock insights\n",
    "        # For now, let's create a test state\n",
    "        test_state = {\n",
    "            \"file_path\": \"test_file.wav\",\n",
    "            \"filename\": \"test_file.wav\",\n",
    "            \"transcript_text\": test_conversation.raw_text,\n",
    "            \"conversation_id\": test_conversation.id,\n",
    "            \"extracted_insights\": None,  # You'd need actual insights here\n",
    "            \"raw_blog_ideas\": None,\n",
    "            \"status\": \"insights_extracted\"\n",
    "        }\n",
    "        \n",
    "        print(\"⚠️ To fully test, you need actual ExtractedInsights object\")\n",
    "        print(\"Run the complete pipeline first to get insights, then test creative agent\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ No suitable test conversation found\")\n",
    "\n",
    "test_creative_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5cabeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AssemblyAI connection successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Test AssemblyAI Connection\n",
    "# Configure AssemblyAI\n",
    "aai.settings.api_key = os.getenv('ASSEMBLYAI_API_KEY')\n",
    "\n",
    "# Test with a simple transcription (we'll use a file from temp folder)\n",
    "def test_assemblyai_connection():\n",
    "    \"\"\"Test if AssemblyAI is working\"\"\"\n",
    "    try:\n",
    "        # Just test the API key is valid\n",
    "        transcriber = aai.Transcriber()\n",
    "        print(\"✅ AssemblyAI connection successful\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ AssemblyAI connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_assemblyai_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01335e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 BATCH PROCESSING INFO:\n",
      "   Files to process: 1\n",
      "   Total size: 27.6 MB\n",
      "\n",
      "📁 Files found:\n",
      "   1. blog_record_hugo_droneflytech.wav (27.6 MB)\n",
      "\n",
      "🚀 Ready to process 1 files!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Batch File Discovery and Management\n",
    "def find_audio_files(temp_folder: Path) -> List[Path]:\n",
    "    \"\"\"Find all audio files in temp folder\"\"\"\n",
    "    audio_extensions = ['*.wav', '*.mp3', '*.m4a']\n",
    "    audio_files = []\n",
    "    \n",
    "    for ext in audio_extensions:\n",
    "        audio_files.extend(temp_folder.glob(ext))\n",
    "    \n",
    "    return sorted(audio_files)\n",
    "\n",
    "def display_batch_info(audio_files: List[Path]):\n",
    "    \"\"\"Display information about the batch of files\"\"\"\n",
    "    if not audio_files:\n",
    "        print(\"❌ No audio files found in temp folder!\")\n",
    "        return False\n",
    "    \n",
    "    total_size_mb = sum(f.stat().st_size for f in audio_files) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"📊 BATCH PROCESSING INFO:\")\n",
    "    print(f\"   Files to process: {len(audio_files)}\")\n",
    "    print(f\"   Total size: {total_size_mb:.1f} MB\")\n",
    "    print(f\"\\n📁 Files found:\")\n",
    "    \n",
    "    for i, file_path in enumerate(audio_files, 1):\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"   {i}. {file_path.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def cleanup_processed_files(processed_files: List[Path]):\n",
    "    \"\"\"Delete all successfully processed files\"\"\"\n",
    "    print(f\"\\n🗑️ CLEANUP: Deleting {len(processed_files)} processed files...\")\n",
    "    deleted_count = 0\n",
    "    \n",
    "    for file_path in processed_files:\n",
    "        try:\n",
    "            file_path.unlink()  # Delete file\n",
    "            print(f\"   ✅ Deleted: {file_path.name}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed to delete {file_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"🗑️ Cleanup complete: {deleted_count}/{len(processed_files)} files deleted\")\n",
    "\n",
    "# Discover files in temp folder\n",
    "temp_folder = project_root / 'data' / 'temp'\n",
    "temp_folder.mkdir(parents=True, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "audio_files = find_audio_files(temp_folder)\n",
    "files_available = display_batch_info(audio_files)\n",
    "\n",
    "if files_available:\n",
    "    print(f\"\\n🚀 Ready to process {len(audio_files)} files!\")\n",
    "else:\n",
    "    print(\"\\n💡 TIP: Add .wav files to data/temp/ folder for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c90af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch processing function ready with full insights display\n"
     ]
    }
   ],
   "source": [
    "# Batch Processing Function (Updated with Full Insights Display)\n",
    "def process_audio_batch(audio_files: List[Path], pipeline) -> dict:\n",
    "    \"\"\"Process all audio files in batch with detailed insights display\"\"\"\n",
    "    \n",
    "    if not audio_files:\n",
    "        print(\"❌ No files to process\")\n",
    "        return {\"processed\": [], \"failed\": [], \"total\": 0}\n",
    "    \n",
    "    print(f\"\\n🚀 STARTING BATCH PROCESSING - {len(audio_files)} files\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    processed_files = []\n",
    "    failed_files = []\n",
    "    results = []\n",
    "    \n",
    "    for i, file_path in enumerate(audio_files, 1):\n",
    "        print(f\"\\n📂 Processing {i}/{len(audio_files)}: {file_path.name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create initial state\n",
    "        initial_state = {\n",
    "            \"file_path\": str(file_path),\n",
    "            \"filename\": file_path.name,\n",
    "            \"transcript_text\": None,\n",
    "            \"conversation_id\": None,\n",
    "            \"extracted_insights\": None,  \n",
    "            \"error\": None,\n",
    "            \"status\": \"processing\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Run through pipeline\n",
    "            result = pipeline.invoke(initial_state)\n",
    "            \n",
    "            if result[\"status\"] in [\"completed\", \"insights_extracted\"]:\n",
    "                print(f\"✅ SUCCESS: {file_path.name}\")\n",
    "                print(f\"   Conversation ID: {result['conversation_id']}\")\n",
    "                print(f\"   Transcript preview: {result['transcript_text'][:100]}...\")\n",
    "                \n",
    "                # FULL INSIGHTS DISPLAY\n",
    "                if result.get('extracted_insights'):\n",
    "                    insights = result['extracted_insights']\n",
    "                    print(f\"\\n🧠 === EXTRACTED INSIGHTS FOR: {file_path.name} ===\")\n",
    "                    print(\"=\" * 50)\n",
    "                    \n",
    "                    # Speakers\n",
    "                    if insights.speakers:\n",
    "                        print(\"👥 SPEAKERS:\")\n",
    "                        for speaker in insights.speakers:\n",
    "                            print(f\"   • Name: {speaker.name or 'Unknown'}\")\n",
    "                            print(f\"     Role: {speaker.role or 'Unknown'}\")  \n",
    "                            print(f\"     Company: {speaker.company or 'Unknown'}\")\n",
    "                    \n",
    "                    # Core Values\n",
    "                    if insights.core_values:\n",
    "                        print(\"💎 CORE VALUES:\")\n",
    "                        for value in insights.core_values:\n",
    "                            print(f\"   • {value}\")\n",
    "                    \n",
    "                    # Priorities\n",
    "                    if insights.priorities:\n",
    "                        print(\"🎯 PRIORITIES:\")\n",
    "                        for priority in insights.priorities:\n",
    "                            print(f\"   • {priority}\")\n",
    "                    \n",
    "                    # Primary Challenges\n",
    "                    if insights.primary_challenges:\n",
    "                        print(\"🔥 PRIMARY CHALLENGES:\")\n",
    "                        for challenge in insights.primary_challenges:\n",
    "                            print(f\"   • Challenge: {challenge.description}\")\n",
    "                            print(f\"     Impact: {challenge.impact}\")\n",
    "                            print(f\"     Urgency: {challenge.urgency}\")\n",
    "                    \n",
    "                    # Secondary Challenges\n",
    "                    if insights.secondary_challenges:\n",
    "                        print(\"⚠️  SECONDARY CHALLENGES:\")\n",
    "                        for challenge in insights.secondary_challenges:\n",
    "                            print(f\"   • Challenge: {challenge.description}\")\n",
    "                            print(f\"     Impact: {challenge.impact}\")\n",
    "                            print(f\"     Urgency: {challenge.urgency}\")\n",
    "                    \n",
    "                    # Current Solutions\n",
    "                    if insights.current_solutions:\n",
    "                        print(\"🔧 CURRENT SOLUTIONS:\")\n",
    "                        for solution in insights.current_solutions:\n",
    "                            print(f\"   • Solution: {solution.solution}\")\n",
    "                            print(f\"     Satisfaction: {solution.satisfaction_level}\")\n",
    "                            if solution.limitations:\n",
    "                                print(f\"     Limitations: {', '.join(solution.limitations)}\")\n",
    "                    \n",
    "                    # Psychological Needs\n",
    "                    if insights.psychological_needs:\n",
    "                        print(\"🧘 PSYCHOLOGICAL NEEDS:\")\n",
    "                        for need in insights.psychological_needs:\n",
    "                            print(f\"   • {need.description}\")\n",
    "                            print(f\"     Category: {need.need_category}\")\n",
    "                            print(f\"     Intensity: {need.intensity}\")\n",
    "                    \n",
    "                    print(\"🧠 === END INSIGHTS ===\")\n",
    "                    print(\"-\" * 50)\n",
    "                \n",
    "                processed_files.append(file_path)\n",
    "            else:\n",
    "                print(f\"❌ FAILED: {file_path.name}\")\n",
    "                print(f\"   Status: {result.get('status', 'Unknown')}\")\n",
    "                print(f\"   Error: {result.get('error', 'Unknown error')}\")\n",
    "                failed_files.append(file_path)\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ PIPELINE ERROR: {file_path.name}\")\n",
    "            print(f\"   Exception: {str(e)}\")\n",
    "            failed_files.append(file_path)\n",
    "            \n",
    "            results.append({\n",
    "                **initial_state,\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"pipeline_error\"\n",
    "            })\n",
    "    \n",
    "    # Final Summary\n",
    "    print(f\"\\n📊 BATCH PROCESSING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"✅ Successfully processed: {len(processed_files)}\")\n",
    "    print(f\"❌ Failed: {len(failed_files)}\")\n",
    "    print(f\"📁 Total files: {len(audio_files)}\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"\\n❌ Failed files:\")\n",
    "        for failed_file in failed_files:\n",
    "            print(f\"   - {failed_file.name}\")\n",
    "    \n",
    "    return {\n",
    "        \"processed\": processed_files,\n",
    "        \"failed\": failed_files,\n",
    "        \"total\": len(audio_files),\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "print(\"✅ Batch processing function ready with full insights display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419ae62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LangGraph nodes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define LangGraph Nodes\n",
    "def transcription_node(state: AudioPipelineState) -> AudioPipelineState:\n",
    "    \"\"\"Node 1: Transcribe audio file with AssemblyAI\"\"\"\n",
    "    try:\n",
    "        print(f\"🎙️ Transcribing: {state['filename']}\")\n",
    "        \n",
    "        # Configure transcriber\n",
    "        transcriber = aai.Transcriber()\n",
    "        \n",
    "        # Transcribe the file\n",
    "        transcript = transcriber.transcribe(state['file_path'])\n",
    "        \n",
    "        if transcript.status == aai.TranscriptStatus.error:\n",
    "            return {\n",
    "                **state,\n",
    "                \"error\": f\"AssemblyAI error: {transcript.error}\",\n",
    "                \"status\": \"transcription_failed\"\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"transcript_text\": transcript.text,\n",
    "            \"status\": \"transcribed\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            \"error\": f\"Transcription error: {str(e)}\",\n",
    "            \"status\": \"transcription_failed\"\n",
    "        }\n",
    "\n",
    "def database_saver_node(state: AudioPipelineState) -> AudioPipelineState:\n",
    "    \"\"\"Node 2: Save transcript to database\"\"\"\n",
    "    try:\n",
    "        print(f\"💾 Saving to database: {state['filename']}\")\n",
    "        \n",
    "        # Create conversation object\n",
    "        conversation = ConversationCreate(\n",
    "            title=f\"Audio: {state['filename']}\",\n",
    "            raw_text=state['transcript_text'],\n",
    "            source=\"transcribed\"\n",
    "        )\n",
    "        \n",
    "        # Save to database\n",
    "        conversation_id = db.create_conversation(conversation)\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"status\": \"completed\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            \"error\": f\"Database error: {str(e)}\",\n",
    "            \"status\": \"database_failed\"\n",
    "        }\n",
    "\n",
    "print(\"✅ LangGraph nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c379c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LangGraph pipeline compiled (3 nodes: transcribe → save_to_db → extract_insights)\n"
     ]
    }
   ],
   "source": [
    "# Cell: Build Current Pipeline (3 Nodes)\n",
    "def build_pipeline():\n",
    "    \"\"\"Build the current LangGraph workflow with transcription, save, and insights\"\"\"\n",
    "    workflow = StateGraph(AudioPipelineState)\n",
    "    \n",
    "    # Add current nodes\n",
    "    workflow.add_node(\"transcribe\", transcription_node)\n",
    "    workflow.add_node(\"save_to_db\", database_saver_node)  \n",
    "    workflow.add_node(\"extract_insights\", pain_extractor_node)\n",
    "    \n",
    "    # Chain them together\n",
    "    workflow.add_edge(\"transcribe\", \"save_to_db\")\n",
    "    workflow.add_edge(\"save_to_db\", \"extract_insights\")\n",
    "    \n",
    "    workflow.set_entry_point(\"transcribe\")\n",
    "    workflow.set_finish_point(\"extract_insights\")\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = build_pipeline()\n",
    "print(\"✅ LangGraph pipeline compiled (3 nodes: transcribe → save_to_db → extract_insights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a45a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Found 0 conversations to delete:\n",
      "❌ Deletion cancelled\n"
     ]
    }
   ],
   "source": [
    "# Cell: Clean Conversations Table\n",
    "def clean_conversations_table():\n",
    "    \"\"\"Delete all records from conversations table\"\"\"\n",
    "    \n",
    "    # First show what will be deleted\n",
    "    conversations = db.get_all_conversations()\n",
    "    print(f\"📊 Found {len(conversations)} conversations to delete:\")\n",
    "    for conv in conversations[:5]:  # Show first 5\n",
    "        print(f\"  - ID {conv.id}: {conv.title}\")\n",
    "    if len(conversations) > 5:\n",
    "        print(f\"  ... and {len(conversations) - 5} more\")\n",
    "    \n",
    "    # Ask for confirmation\n",
    "    response = input(f\"\\n❓ Delete all {len(conversations)} conversations? (y/N): \")\n",
    "    \n",
    "    if response.lower() in ['y', 'yes']:\n",
    "        conn = db.get_connection()\n",
    "        try:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Delete all conversations (this will also delete related blog_post_ideas due to foreign key)\n",
    "            cursor.execute(\"DELETE FROM blog_post_ideas\")\n",
    "            cursor.execute(\"DELETE FROM processing_status\") \n",
    "            cursor.execute(\"DELETE FROM conversations\")\n",
    "            conn.commit()\n",
    "            \n",
    "            print(\"✅ All conversations deleted!\")\n",
    "            print(\"✅ Related blog ideas deleted!\")\n",
    "            print(\"✅ Processing status cleared!\")\n",
    "            \n",
    "        finally:\n",
    "            conn.close()\n",
    "    else:\n",
    "        print(\"❌ Deletion cancelled\")\n",
    "\n",
    "# Run the cleaner\n",
    "clean_conversations_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cb00a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting batch processing...\n",
      "\n",
      "🚀 STARTING BATCH PROCESSING - 1 files\n",
      "============================================================\n",
      "\n",
      "📂 Processing 1/1: blog_record_hugo_droneflytech.wav\n",
      "----------------------------------------\n",
      "🎙️ Transcribing: blog_record_hugo_droneflytech.wav\n",
      "💾 Saving to database: blog_record_hugo_droneflytech.wav\n",
      "🧠 Starting pain extraction...\n",
      "✅ Extracted insights: 1 primary challenges, 1 speakers\n",
      "✅ SUCCESS: blog_record_hugo_droneflytech.wav\n",
      "   Conversation ID: 9\n",
      "   Transcript preview: At Drone Fly Tech, one of our primary challenges revolves around tracking and knowing who paid which...\n",
      "\n",
      "🧠 === EXTRACTED INSIGHTS FOR: blog_record_hugo_droneflytech.wav ===\n",
      "==================================================\n",
      "👥 SPEAKERS:\n",
      "   • Name: Hugo\n",
      "     Role: SpeakerRole.CLIENT\n",
      "     Company: Drone flytech\n",
      "💎 CORE VALUES:\n",
      "   • efficiency\n",
      "   • transparency\n",
      "   • accuracy\n",
      "🎯 PRIORITIES:\n",
      "   • improving financial processes\n",
      "   • streamlining invoice tracking\n",
      "   • enhancing financial management\n",
      "🔥 PRIMARY CHALLENGES:\n",
      "   • Challenge: Tracking who paid which invoice\n",
      "     Impact: Creates confusion and delays in financial processes\n",
      "     Urgency: High\n",
      "⚠️  SECONDARY CHALLENGES:\n",
      "   • Challenge: Managing multiple transactions with expanding client base\n",
      "     Impact: Increased complexity in financial tracking\n",
      "     Urgency: Medium\n",
      "🔧 CURRENT SOLUTIONS:\n",
      "   • Solution: Using MoneyOak software\n",
      "     Satisfaction: Unsatisfied\n",
      "     Limitations: inadequate functionality, limited visibility, lack of detailed information\n",
      "🧘 PSYCHOLOGICAL NEEDS:\n",
      "   • Confidence in financial operations\n",
      "     Category: security\n",
      "     Intensity: High\n",
      "   • Peace of mind in financial management\n",
      "     Category: control\n",
      "     Intensity: High\n",
      "🧠 === END INSIGHTS ===\n",
      "--------------------------------------------------\n",
      "\n",
      "📊 BATCH PROCESSING COMPLETE!\n",
      "============================================================\n",
      "✅ Successfully processed: 1\n",
      "❌ Failed: 0\n",
      "📁 Total files: 1\n",
      "\n",
      "📊 BATCH PROCESSING COMPLETE!\n",
      "============================================================\n",
      "✅ Successfully processed: 1\n",
      "❌ Failed: 0\n",
      "📁 Total files: 1\n",
      "\n",
      "🗑️ CLEANUP: Deleting 1 processed files...\n",
      "   ✅ Deleted: blog_record_hugo_droneflytech.wav\n",
      "🗑️ Cleanup complete: 1/1 files deleted\n",
      "\n",
      "🎉 Batch processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Execute Batch Processing with Cleanup\n",
    "if files_available:\n",
    "    print(\"🎯 Starting batch processing...\")\n",
    "    \n",
    "    # Process all files\n",
    "    batch_results = process_audio_batch(audio_files, pipeline)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\n📊 BATCH PROCESSING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"✅ Successfully processed: {len(batch_results['processed'])}\")\n",
    "    print(f\"❌ Failed: {len(batch_results['failed'])}\")\n",
    "    print(f\"📁 Total files: {batch_results['total']}\")\n",
    "    \n",
    "    # Show failed files\n",
    "    if batch_results['failed']:\n",
    "        print(f\"\\n❌ Failed files:\")\n",
    "        for file_path in batch_results['failed']:\n",
    "            print(f\"   - {file_path.name}\")\n",
    "    \n",
    "    # Cleanup successfully processed files\n",
    "    if batch_results['processed']:\n",
    "        confirm = input(f\"\\n🗑️ Delete {len(batch_results['processed'])} processed files? (y/N): \")\n",
    "        if confirm.lower() in ['y', 'yes']:\n",
    "            cleanup_processed_files(batch_results['processed'])\n",
    "        else:\n",
    "            print(\"🔧 Files kept in temp folder for inspection\")\n",
    "    \n",
    "    print(\"\\n🎉 Batch processing complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"💡 Add audio files to data/temp/ folder and rerun this cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeaef0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Anthropic LLM initialized with Claude 3.5 Sonnet\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Setup Anthropic LLM for Insights Extraction (FIXED)\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "import json\n",
    "\n",
    "# Initialize Anthropic with correct model name\n",
    "anthropic_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not anthropic_key:\n",
    "    print(\"⚠️  ANTHROPIC_API_KEY not found in .env file\")\n",
    "    print(\"Please add: ANTHROPIC_API_KEY=your_key_here\")\n",
    "else:\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-3-5-sonnet-20241022\",  # ← Updated model name\n",
    "        api_key=anthropic_key,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    print(\"✅ Anthropic LLM initialized with Claude 3.5 Sonnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b3cea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. PainExtractor Node Implementation\n",
    "\n",
    "\n",
    "import openai\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "# System prompt\n",
    "PAIN_EXTRACTOR_SYSTEM_PROMPT = \"\"\"\n",
    "You are a UX researcher and business analyst for BigKids Automation. Your job is listening to transcripts from interviews with users and potential clients. \n",
    "\n",
    "You pay special attention to problems that users have regarding how their company is automating, using web apps and AI to save time and move towards a more ethical and sovereign tech infrastructure.\n",
    "\n",
    "You will be given the transcript of an interview with a user or potential client.\n",
    "\n",
    "Your task is to extract structured information about:\n",
    "- Who is speaking and their role\n",
    "- What this person cares about (values, priorities)\n",
    "- Their main primary and secondary challenges\n",
    "- How they are solving problems today\n",
    "- Are there AI agents that can assist them?\n",
    "- Their underlying psychological needs (using frameworks like NVC - Non-Violent Communication)\n",
    "\n",
    "Focus on automation, web apps, AI, time-saving, ethical tech, and sovereign infrastructure themes.\n",
    "\n",
    "Be thorough but concise. \n",
    "\n",
    "IMPORTANT: Only extract information that is explicitly mentioned in the transcript. \n",
    "If information is not clearly stated, leave the field empty/null rather than guessing or inferring.\n",
    "Do not hallucinate or make assumptions about missing information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "819a4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_insights_from_transcript(transcript: str) -> ExtractedInsights:\n",
    "    \"\"\"Extract structured insights using Anthropic Claude with proper JSON structure\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze this conversation transcript and extract structured insights in the exact JSON format below:\n",
    "\n",
    "    Transcript: {transcript}\n",
    "\n",
    "    Return ONLY valid JSON in this exact structure:\n",
    "    {{\n",
    "        \"speakers\": [\n",
    "            {{\n",
    "                \"name\": \"Hugo\",\n",
    "                \"role\": \"client\", \n",
    "                \"company\": \"Drone flytech\"\n",
    "            }}\n",
    "        ],\n",
    "        \"core_values\": [\"efficiency\", \"transparency\"],\n",
    "        \"priorities\": [\"improving financial processes\"],\n",
    "        \"primary_challenges\": [\n",
    "            {{\n",
    "                \"description\": \"Tracking who paid which invoice\",\n",
    "                \"impact\": \"Creates confusion in financial processes\",\n",
    "                \"urgency\": \"High\"\n",
    "            }}\n",
    "        ],\n",
    "        \"secondary_challenges\": [],\n",
    "        \"current_solutions\": [\n",
    "            {{\n",
    "                \"solution\": \"Using MoneyOak software\",\n",
    "                \"satisfaction_level\": \"Unsatisfied\", \n",
    "                \"limitations\": [\"inadequate functionality\", \"limited visibility\"]\n",
    "            }}\n",
    "        ],\n",
    "        \"psychological_needs\": [\n",
    "            {{\n",
    "                \"need_category\": \"security\",\n",
    "                \"description\": \"Confidence in financial operations\",\n",
    "                \"intensity\": \"High\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Important: \n",
    "    - Return ONLY the JSON, no other text\n",
    "    - Use exact field names as shown\n",
    "    - For urgency use: \"Low\", \"Medium\", or \"High\"\n",
    "    - For satisfaction_level use: \"Very Satisfied\", \"Satisfied\", \"Neutral\", \"Unsatisfied\", \"Very Unsatisfied\"\n",
    "    - For intensity use: \"Low\", \"Medium\", or \"High\"\n",
    "    - For speaker role use: \"client\" or \"interviewer\"\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Use the Claude LLM\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        insights_data = json.loads(response.content)\n",
    "        \n",
    "        # Convert to Pydantic model\n",
    "        return ExtractedInsights(**insights_data)\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ JSON parsing error: {e}\")\n",
    "        print(f\"📝 Raw response: {response.content[:500]}...\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in LLM call: {e}\")\n",
    "        raise\n",
    "def pain_extractor_node(state: AudioPipelineState) -> AudioPipelineState:\n",
    "    \"\"\"\n",
    "    LangGraph node: Extract structured insights from conversation transcript\n",
    "    \"\"\"\n",
    "    print(\"🧠 Starting pain extraction...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract insights using OpenAI structured output\n",
    "        insights = extract_insights_from_transcript(state['transcript_text'])\n",
    "        \n",
    "        if insights:\n",
    "            print(f\"✅ Extracted insights: {len(insights.primary_challenges)} primary challenges, {len(insights.speakers)} speakers\")\n",
    "            \n",
    "            return {\n",
    "                **state,\n",
    "                \"extracted_insights\": insights,\n",
    "                \"status\": \"insights_extracted\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                **state,\n",
    "                \"error\": \"Failed to extract insights from transcript\",\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Pain extraction failed: {e}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"error\": f\"Pain extraction error: {str(e)}\",\n",
    "            \"status\": \"error\"\n",
    "        }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
