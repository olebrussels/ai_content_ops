{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb001904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AssemblyAI API Key loaded: ‚úÖ\n",
      "Key starts with: 972365f41d...\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(project_root / '.env')\n",
    "\n",
    "# Test API key\n",
    "assemblyai_key = os.getenv('ASSEMBLYAI_API_KEY')\n",
    "print(f\"AssemblyAI API Key loaded: {'‚úÖ' if assemblyai_key else '‚ùå'}\")\n",
    "print(f\"Key starts with: {assemblyai_key[:10] if assemblyai_key else 'None'}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42981215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "944e386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ai_content_ops\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc1fee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Tables in app.db:\n",
      "\n",
      "üîß conversations:\n",
      "   - id (INTEGER)\n",
      "   - title (TEXT)\n",
      "   - raw_text (TEXT)\n",
      "   - source (TEXT)\n",
      "   - word_count (INTEGER)\n",
      "   - created_at (DATETIME)\n",
      "   - status (TEXT)\n",
      "\n",
      "üîß sqlite_sequence:\n",
      "   - name ()\n",
      "   - seq ()\n",
      "\n",
      "üîß blog_post_ideas:\n",
      "   - id (INTEGER)\n",
      "   - conversation_id (INTEGER)\n",
      "   - title (TEXT)\n",
      "   - description (TEXT)\n",
      "   - usefulness_potential (INTEGER)\n",
      "   - fitwith_seo_strategy (INTEGER)\n",
      "   - fitwith_content_strategy (INTEGER)\n",
      "   - inspiration_potential (INTEGER)\n",
      "   - collaboration_potential (INTEGER)\n",
      "   - innovation (INTEGER)\n",
      "   - difficulty (INTEGER)\n",
      "   - total_score (INTEGER)\n",
      "   - sent_to_prod (BOOLEAN)\n",
      "   - raw_llm_response (TEXT)\n",
      "   - created_at (DATETIME)\n",
      "\n",
      "üîß processing_status:\n",
      "   - id (INTEGER)\n",
      "   - conversation_id (INTEGER)\n",
      "   - stage (TEXT)\n",
      "   - status (TEXT)\n",
      "   - error_message (TEXT)\n",
      "   - started_at (DATETIME)\n",
      "   - completed_at (DATETIME)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"data/app.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get all table names\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "print(\"üìä Tables in app.db:\")\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    \n",
    "    # Get column info for each table\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    columns = cursor.fetchall()\n",
    "    \n",
    "    print(f\"\\nüîß {table_name}:\")\n",
    "    for col in columns:\n",
    "        print(f\"   - {col[1]} ({col[2]})\")  # column_name (type)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f9e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Dependencies\n",
    "import assemblyai as aai\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict, Optional, List, Dict  # ‚Üê Added List here\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path  # ‚Üê Also added Path here\n",
    "\n",
    "# Import our database\n",
    "from database.db_operations import db\n",
    "from database.models import ConversationCreate\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30147df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Pydantic Model for Structured Output\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class SpeakerRole(str, Enum):\n",
    "    \"\"\"Possible speaker roles in the conversation\"\"\"\n",
    "    CLIENT = \"client\"\n",
    "    INTERVIEWER = \"interviewer\"\n",
    "\n",
    "class Speaker(BaseModel):\n",
    "    \"\"\"Information about a person speaking in the conversation\"\"\"\n",
    "    name: Optional[str] = Field(default=None, description=\"Name of the speaker if mentioned\")\n",
    "    role: Optional[SpeakerRole] = Field(default=None, description=\"Role of the speaker in the conversation\")\n",
    "    company: Optional[str] = Field(default=None, description=\"Company they work for if mentioned\")\n",
    "\n",
    "class Challenge(BaseModel):\n",
    "    \"\"\"A challenge or problem mentioned in the conversation\"\"\"\n",
    "    description: Optional[str] = Field(default=None, description=\"Description of the challenge\")\n",
    "    impact: Optional[str] = Field(default=None, description=\"How this challenge affects them\")\n",
    "    urgency: Optional[str] = Field(default=None, description=\"Low, Medium, or High urgency\")\n",
    "\n",
    "class CurrentSolution(BaseModel):\n",
    "    \"\"\"How they currently solve their problems\"\"\"\n",
    "    solution: Optional[str] = Field(default=None, description=\"What they're currently doing\")\n",
    "    satisfaction_level: Optional[str] = Field(default=None, description=\"How satisfied they are: Very Satisfied, Satisfied, Neutral, Unsatisfied, Very Unsatisfied\")\n",
    "    limitations: Optional[List[str]] = Field(default=[], description=\"Limitations of current solution\")\n",
    "\n",
    "class Need(BaseModel):\n",
    "    \"\"\"A need identified using psychology frameworks like NVC\"\"\"\n",
    "    need_category: Optional[str] = Field(default=None, description=\"Category of need (e.g., autonomy, efficiency, security, connection)\")\n",
    "    description: Optional[str] = Field(default=None, description=\"Specific need description\")\n",
    "    intensity: Optional[str] = Field(default=None, description=\"Low, Medium, or High intensity\")\n",
    "\n",
    "class ExtractedInsights(BaseModel):\n",
    "    \"\"\"Complete structured output from conversation analysis\"\"\"\n",
    "    \n",
    "    # Speakers\n",
    "    speakers: Optional[List[Speaker]] = Field(default=[], description=\"People identified in the conversation\")\n",
    "    \n",
    "    # What they care about\n",
    "    core_values: Optional[List[str]] = Field(default=[], description=\"What this person/company cares about most\")\n",
    "    priorities: Optional[List[str]] = Field(default=[], description=\"Their current priorities and focus areas\")\n",
    "    \n",
    "    # Challenges\n",
    "    primary_challenges: Optional[List[Challenge]] = Field(default=[], description=\"Main problems they're facing\")\n",
    "    secondary_challenges: Optional[List[Challenge]] = Field(default=[], description=\"Secondary or related problems\")\n",
    "    \n",
    "    # Current solutions\n",
    "    current_solutions: Optional[List[CurrentSolution]] = Field(default=[], description=\"How they solve problems today\")\n",
    "    \n",
    "    # Needs analysis\n",
    "    psychological_needs: Optional[List[Need]] = Field(default=[], description=\"Underlying needs using NVC or similar frameworks\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d86360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Simple RawBlogIdea model ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Raw Blog Idea Model (Simple)\n",
    "class RawBlogIdea(BaseModel):\n",
    "    \"\"\"Raw blog idea from creative agent\"\"\"\n",
    "    title: str\n",
    "    description: str\n",
    "    target_audience: str\n",
    "    content_angle: str\n",
    "    business_value: str\n",
    "\n",
    "print(\"‚úÖ Simple RawBlogIdea model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ecdfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RawBlogIdea model and validation ready\n"
     ]
    }
   ],
   "source": [
    "def validate_raw_blog_ideas(raw_ideas: List[Dict]) -> List[RawBlogIdea]:\n",
    "    \"\"\"Validate and convert raw JSON to Pydantic models\"\"\"\n",
    "    validated_ideas = []\n",
    "    \n",
    "    for idea in raw_ideas:\n",
    "        try:\n",
    "            validated_idea = RawBlogIdea(**idea)\n",
    "            validated_ideas.append(validated_idea)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Invalid blog idea skipped: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Validated {len(validated_ideas)} out of {len(raw_ideas)} raw ideas\")\n",
    "    return validated_ideas\n",
    "\n",
    "print(\"‚úÖ RawBlogIdea model and validation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19fad4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ State defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define LangGraph State\n",
    "class AudioPipelineState(TypedDict):\n",
    "    file_path: str\n",
    "    filename: str\n",
    "    transcript_text: Optional[str]\n",
    "    conversation_id: Optional[int]\n",
    "    extracted_insights: Optional[ExtractedInsights]  \n",
    "    raw_blog_ideas: Optional[List[Dict]]           # Pydantic objects from creative agent\n",
    "\n",
    "    \n",
    "    # Status & error handling\n",
    "    status: str\n",
    "    error: Optional[str]\n",
    "\n",
    "\n",
    "print(\"‚úÖ State defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8850b086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded company strategy (6555 chars)\n",
      "‚úÖ Loaded SEO strategy (5566 chars)\n",
      "üìä Strategy context keys: ['company_strategy', 'seo_strategy']\n"
     ]
    }
   ],
   "source": [
    "#ad Company Strategy Context\n",
    "def load_company_strategy_context():\n",
    "    \"\"\"Load company strategy and SEO strategy for creative context\"\"\"\n",
    "    \n",
    "    strategy_context = {}\n",
    "    \n",
    "    try:\n",
    "        # Load company strategy\n",
    "        company_strategy_path = \"../data/processed/company_strategy.mkd\"\n",
    "        with open(company_strategy_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            strategy_context[\"company_strategy\"] = f.read()\n",
    "        print(f\"‚úÖ Loaded company strategy ({len(strategy_context['company_strategy'])} chars)\")\n",
    "        \n",
    "        # Load SEO strategy if exists\n",
    "        seo_strategy_path = \"../data/processed/seo_strategy.mkd\"\n",
    "        if os.path.exists(seo_strategy_path):\n",
    "            with open(seo_strategy_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                strategy_context[\"seo_strategy\"] = f.read()\n",
    "            print(f\"‚úÖ Loaded SEO strategy ({len(strategy_context['seo_strategy'])} chars)\")\n",
    "        else:\n",
    "            strategy_context[\"seo_strategy\"] = \"No specific SEO strategy document found.\"\n",
    "            print(\"‚ö†Ô∏è No SEO strategy document found, using default guidance\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading strategy documents: {e}\")\n",
    "        strategy_context = {\n",
    "            \"company_strategy\": \"Strategy document not available\",\n",
    "            \"seo_strategy\": \"SEO strategy document not available\"\n",
    "        }\n",
    "    \n",
    "    return strategy_context\n",
    "\n",
    "# Test loading\n",
    "strategy_context = load_company_strategy_context()\n",
    "print(f\"üìä Strategy context keys: {list(strategy_context.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1b52ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fixed creative agent function ready\n"
     ]
    }
   ],
   "source": [
    "# Cell: Fixed Creative Agent Function\n",
    "def generate_blog_ideas_from_insights(insights: ExtractedInsights, strategy_context: dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fixed creative agent that handles Claude's markdown JSON response\n",
    "    \"\"\"\n",
    "    \n",
    "    creative_prompt = f\"\"\"\n",
    "    You are a creative content strategist for Big Kids Automation, a company that helps businesses implement AI and automation solutions.\n",
    "    \n",
    "    COMPANY CONTEXT:\n",
    "    {strategy_context.get('company_strategy', 'Strategy not available')[:1000]}...\n",
    "    \n",
    "    SEO STRATEGY:\n",
    "    {strategy_context.get('seo_strategy', 'SEO strategy not available')[:500]}...\n",
    "    \n",
    "    CONVERSATION INSIGHTS TO WORK FROM:\n",
    "    \n",
    "    Speakers: {[f\"{s.name} ({s.role}) from {s.company}\" for s in insights.speakers] if insights.speakers else \"Unknown speakers\"}\n",
    "    \n",
    "    Core Values: {\", \".join(insights.core_values) if insights.core_values else \"None identified\"}\n",
    "    \n",
    "    Priorities: {\", \".join(insights.priorities) if insights.priorities else \"None identified\"}\n",
    "    \n",
    "    Primary Challenges:\n",
    "    {chr(10).join([f\"- {c.description} (Impact: {c.impact}, Urgency: {c.urgency})\" for c in insights.primary_challenges]) if insights.primary_challenges else \"None identified\"}\n",
    "    \n",
    "    Current Solutions:\n",
    "    {chr(10).join([f\"- {s.solution} (Satisfaction: {s.satisfaction_level})\" for s in insights.current_solutions]) if insights.current_solutions else \"None identified\"}\n",
    "    \n",
    "    Psychological Needs:\n",
    "    {chr(10).join([f\"- {n.description} ({n.need_category}, {n.intensity} intensity)\" for n in insights.psychological_needs]) if insights.psychological_needs else \"None identified\"}\n",
    "    \n",
    "    TASK:\n",
    "    Generate 4-5 creative blog post ideas that:\n",
    "    1. Address the challenges and needs identified in this conversation\n",
    "    2. Align with Big Kids Automation's mission to help businesses with AI/automation\n",
    "    3. Provide value to potential clients facing similar challenges\n",
    "    4. Support our SEO and content marketing strategy\n",
    "    5. Are actionable and practical, not just theoretical\n",
    "    \n",
    "    For each blog post idea, provide:\n",
    "    - title: Clear, engaging title that includes relevant keywords\n",
    "    - description: 2-3 sentence description of what the post will cover\n",
    "    - target_audience: Who this post is primarily for\n",
    "    - content_angle: The unique angle or approach this post takes\n",
    "    - business_value: How this post helps our business goals\n",
    "    \n",
    "    IMPORTANT: Return ONLY the JSON array, no markdown formatting, no code blocks, no explanatory text.\n",
    "    \n",
    "    Format:\n",
    "    [\n",
    "        {{\n",
    "            \"title\": \"How AI Proposal Systems Balance Speed with Brand Differentiation\",\n",
    "            \"description\": \"A practical guide showing how modern AI-powered proposal systems solve the common problem of maintaining company uniqueness while leveraging automation. Includes real case studies and implementation steps.\",\n",
    "            \"target_audience\": \"Business development directors and proposal managers at consulting firms\",\n",
    "            \"content_angle\": \"Problem-solution with real case studies\",\n",
    "            \"business_value\": \"Attracts prospects struggling with proposal automation while maintaining differentiation\"\n",
    "        }}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Generate ideas using Claude\n",
    "        response = llm.invoke(creative_prompt)\n",
    "        raw_content = response.content.strip()\n",
    "        \n",
    "        print(f\"üìù Raw response length: {len(raw_content)} chars\")\n",
    "        print(f\"üìù Response starts with: {raw_content[:50]}...\")\n",
    "        \n",
    "        # Handle markdown code blocks\n",
    "        if raw_content.startswith('```'):\n",
    "            print(\"üîß Removing markdown code blocks...\")\n",
    "            # Remove ```json and ``` wrappers\n",
    "            lines = raw_content.split('\\n')\n",
    "            # Remove first line if it's ```json or ```\n",
    "            if lines[0].startswith('```'):\n",
    "                lines = lines[1:]\n",
    "            # Remove last line if it's ```\n",
    "            if lines and lines[-1].strip() == '```':\n",
    "                lines = lines[:-1]\n",
    "            raw_content = '\\n'.join(lines).strip()\n",
    "            print(f\"üîß Cleaned content starts with: {raw_content[:50]}...\")\n",
    "        \n",
    "        # Parse JSON response\n",
    "        blog_ideas = json.loads(raw_content)\n",
    "        \n",
    "        print(f\"‚úÖ Creative agent successfully parsed {len(blog_ideas)} blog ideas\")\n",
    "        return blog_ideas\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON parsing error in creative agent: {e}\")\n",
    "        print(f\"üìù Cleaned content: {raw_content[:500]}...\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in creative agent: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Fixed creative agent function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a3c3251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Creative agent node (direct Pydantic) ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Creative Agent Node (Direct Pydantic)\n",
    "def creative_agent_node(state: AudioPipelineState) -> AudioPipelineState:\n",
    "    \"\"\"Creative agent that generates raw blog ideas as Pydantic objects\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üé® Starting creative blog idea generation...\")\n",
    "        \n",
    "        insights = state.get('extracted_insights')\n",
    "        if not insights:\n",
    "            return {**state, \"error\": \"No insights available\", \"status\": \"error\"}\n",
    "        \n",
    "        # Load strategy context\n",
    "        strategy_context = load_company_strategy_context()\n",
    "        \n",
    "        # Generate ideas (returns JSON)\n",
    "        raw_ideas_json = generate_blog_ideas_from_insights(insights, strategy_context)\n",
    "        \n",
    "        # Convert directly to Pydantic objects\n",
    "        raw_blog_ideas = []\n",
    "        for idea_json in raw_ideas_json:\n",
    "            try:\n",
    "                idea = RawBlogIdea(**idea_json)\n",
    "                raw_blog_ideas.append(idea)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping invalid idea: {e}\")\n",
    "        \n",
    "        if raw_blog_ideas:\n",
    "            print(f\"üéâ Generated {len(raw_blog_ideas)} valid blog ideas\")\n",
    "            return {\n",
    "                **state,\n",
    "                \"raw_blog_ideas\": raw_blog_ideas,  # Direct Pydantic objects\n",
    "                \"status\": \"raw_ideas_generated\"\n",
    "            }\n",
    "        else:\n",
    "            return {**state, \"error\": \"No valid ideas generated\", \"status\": \"error\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Creative agent error: {e}\")\n",
    "        return {**state, \"error\": str(e), \"status\": \"error\"}\n",
    "\n",
    "print(\"‚úÖ Creative agent node (direct Pydantic) ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5cabeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AssemblyAI connection successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Test AssemblyAI Connection\n",
    "# Configure AssemblyAI\n",
    "aai.settings.api_key = os.getenv('ASSEMBLYAI_API_KEY')\n",
    "\n",
    "# Test with a simple transcription (we'll use a file from temp folder)\n",
    "def test_assemblyai_connection():\n",
    "    \"\"\"Test if AssemblyAI is working\"\"\"\n",
    "    try:\n",
    "        # Just test the API key is valid\n",
    "        transcriber = aai.Transcriber()\n",
    "        print(\"‚úÖ AssemblyAI connection successful\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå AssemblyAI connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_assemblyai_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01335e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No audio files found in temp folder!\n",
      "\n",
      "üí° TIP: Add .wav files to data/temp/ folder for testing\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Batch File Discovery and Management\n",
    "def find_audio_files(temp_folder: Path) -> List[Path]:\n",
    "    \"\"\"Find all audio files in temp folder\"\"\"\n",
    "    audio_extensions = ['*.wav', '*.mp3', '*.m4a']\n",
    "    audio_files = []\n",
    "    \n",
    "    for ext in audio_extensions:\n",
    "        audio_files.extend(temp_folder.glob(ext))\n",
    "    \n",
    "    return sorted(audio_files)\n",
    "\n",
    "def display_batch_info(audio_files: List[Path]):\n",
    "    \"\"\"Display information about the batch of files\"\"\"\n",
    "    if not audio_files:\n",
    "        print(\"‚ùå No audio files found in temp folder!\")\n",
    "        return False\n",
    "    \n",
    "    total_size_mb = sum(f.stat().st_size for f in audio_files) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"üìä BATCH PROCESSING INFO:\")\n",
    "    print(f\"   Files to process: {len(audio_files)}\")\n",
    "    print(f\"   Total size: {total_size_mb:.1f} MB\")\n",
    "    print(f\"\\nüìÅ Files found:\")\n",
    "    \n",
    "    for i, file_path in enumerate(audio_files, 1):\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"   {i}. {file_path.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def cleanup_processed_files(processed_files: List[Path]):\n",
    "    \"\"\"Delete all successfully processed files\"\"\"\n",
    "    print(f\"\\nüóëÔ∏è CLEANUP: Deleting {len(processed_files)} processed files...\")\n",
    "    deleted_count = 0\n",
    "    \n",
    "    for file_path in processed_files:\n",
    "        try:\n",
    "            file_path.unlink()  # Delete file\n",
    "            print(f\"   ‚úÖ Deleted: {file_path.name}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed to delete {file_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"üóëÔ∏è Cleanup complete: {deleted_count}/{len(processed_files)} files deleted\")\n",
    "\n",
    "# Discover files in temp folder\n",
    "temp_folder = project_root / 'data' / 'temp'\n",
    "temp_folder.mkdir(parents=True, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "audio_files = find_audio_files(temp_folder)\n",
    "files_available = display_batch_info(audio_files)\n",
    "\n",
    "if files_available:\n",
    "    print(f\"\\nüöÄ Ready to process {len(audio_files)} files!\")\n",
    "else:\n",
    "    print(\"\\nüí° TIP: Add .wav files to data/temp/ folder for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c90af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch processing function ready with full insights display\n"
     ]
    }
   ],
   "source": [
    "# Batch Processing Function (Updated with Full Insights Display)\n",
    "def process_audio_batch(audio_files: List[Path], pipeline) -> dict:\n",
    "    \"\"\"Process all audio files in batch with detailed insights display\"\"\"\n",
    "    \n",
    "    if not audio_files:\n",
    "        print(\"‚ùå No files to process\")\n",
    "        return {\"processed\": [], \"failed\": [], \"total\": 0}\n",
    "    \n",
    "    print(f\"\\nüöÄ STARTING BATCH PROCESSING - {len(audio_files)} files\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    processed_files = []\n",
    "    failed_files = []\n",
    "    results = []\n",
    "    \n",
    "    for i, file_path in enumerate(audio_files, 1):\n",
    "        print(f\"\\nüìÇ Processing {i}/{len(audio_files)}: {file_path.name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create initial state\n",
    "        initial_state = {\n",
    "            \"file_path\": str(file_path),\n",
    "            \"filename\": file_path.name,\n",
    "            \"transcript_text\": None,\n",
    "            \"conversation_id\": None,\n",
    "            \"extracted_insights\": None,  \n",
    "            \"error\": None,\n",
    "            \"status\": \"processing\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Run through pipeline\n",
    "            result = pipeline.invoke(initial_state)\n",
    "            \n",
    "            if result[\"status\"] in [\"completed\", \"insights_extracted\"]:\n",
    "                print(f\"‚úÖ SUCCESS: {file_path.name}\")\n",
    "                print(f\"   Conversation ID: {result['conversation_id']}\")\n",
    "                print(f\"   Transcript preview: {result['transcript_text'][:100]}...\")\n",
    "                \n",
    "                # FULL INSIGHTS DISPLAY\n",
    "                if result.get('extracted_insights'):\n",
    "                    insights = result['extracted_insights']\n",
    "                    print(f\"\\nüß† === EXTRACTED INSIGHTS FOR: {file_path.name} ===\")\n",
    "                    print(\"=\" * 50)\n",
    "                    \n",
    "                    # Speakers\n",
    "                    if insights.speakers:\n",
    "                        print(\"üë• SPEAKERS:\")\n",
    "                        for speaker in insights.speakers:\n",
    "                            print(f\"   ‚Ä¢ Name: {speaker.name or 'Unknown'}\")\n",
    "                            print(f\"     Role: {speaker.role or 'Unknown'}\")  \n",
    "                            print(f\"     Company: {speaker.company or 'Unknown'}\")\n",
    "                    \n",
    "                    # Core Values\n",
    "                    if insights.core_values:\n",
    "                        print(\"üíé CORE VALUES:\")\n",
    "                        for value in insights.core_values:\n",
    "                            print(f\"   ‚Ä¢ {value}\")\n",
    "                    \n",
    "                    # Priorities\n",
    "                    if insights.priorities:\n",
    "                        print(\"üéØ PRIORITIES:\")\n",
    "                        for priority in insights.priorities:\n",
    "                            print(f\"   ‚Ä¢ {priority}\")\n",
    "                    \n",
    "                    # Primary Challenges\n",
    "                    if insights.primary_challenges:\n",
    "                        print(\"üî• PRIMARY CHALLENGES:\")\n",
    "                        for challenge in insights.primary_challenges:\n",
    "                            print(f\"   ‚Ä¢ Challenge: {challenge.description}\")\n",
    "                            print(f\"     Impact: {challenge.impact}\")\n",
    "                            print(f\"     Urgency: {challenge.urgency}\")\n",
    "                    \n",
    "                    # Secondary Challenges\n",
    "                    if insights.secondary_challenges:\n",
    "                        print(\"‚ö†Ô∏è  SECONDARY CHALLENGES:\")\n",
    "                        for challenge in insights.secondary_challenges:\n",
    "                            print(f\"   ‚Ä¢ Challenge: {challenge.description}\")\n",
    "                            print(f\"     Impact: {challenge.impact}\")\n",
    "                            print(f\"     Urgency: {challenge.urgency}\")\n",
    "                    \n",
    "                    # Current Solutions\n",
    "                    if insights.current_solutions:\n",
    "                        print(\"üîß CURRENT SOLUTIONS:\")\n",
    "                        for solution in insights.current_solutions:\n",
    "                            print(f\"   ‚Ä¢ Solution: {solution.solution}\")\n",
    "                            print(f\"     Satisfaction: {solution.satisfaction_level}\")\n",
    "                            if solution.limitations:\n",
    "                                print(f\"     Limitations: {', '.join(solution.limitations)}\")\n",
    "                    \n",
    "                    # Psychological Needs\n",
    "                    if insights.psychological_needs:\n",
    "                        print(\"üßò PSYCHOLOGICAL NEEDS:\")\n",
    "                        for need in insights.psychological_needs:\n",
    "                            print(f\"   ‚Ä¢ {need.description}\")\n",
    "                            print(f\"     Category: {need.need_category}\")\n",
    "                            print(f\"     Intensity: {need.intensity}\")\n",
    "                    \n",
    "                    print(\"üß† === END INSIGHTS ===\")\n",
    "                    print(\"-\" * 50)\n",
    "                \n",
    "                processed_files.append(file_path)\n",
    "            else:\n",
    "                print(f\"‚ùå FAILED: {file_path.name}\")\n",
    "                print(f\"   Status: {result.get('status', 'Unknown')}\")\n",
    "                print(f\"   Error: {result.get('error', 'Unknown error')}\")\n",
    "                failed_files.append(file_path)\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå PIPELINE ERROR: {file_path.name}\")\n",
    "            print(f\"   Exception: {str(e)}\")\n",
    "            failed_files.append(file_path)\n",
    "            \n",
    "            results.append({\n",
    "                **initial_state,\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"pipeline_error\"\n",
    "            })\n",
    "    \n",
    "    # Final Summary\n",
    "    print(f\"\\nüìä BATCH PROCESSING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Successfully processed: {len(processed_files)}\")\n",
    "    print(f\"‚ùå Failed: {len(failed_files)}\")\n",
    "    print(f\"üìÅ Total files: {len(audio_files)}\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"\\n‚ùå Failed files:\")\n",
    "        for failed_file in failed_files:\n",
    "            print(f\"   - {failed_file.name}\")\n",
    "    \n",
    "    return {\n",
    "        \"processed\": processed_files,\n",
    "        \"failed\": failed_files,\n",
    "        \"total\": len(audio_files),\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Batch processing function ready with full insights display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "419ae62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangGraph nodes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define LangGraph Nodes\n",
    "def transcription_node(state: AudioPipelineState) -> AudioPipelineState:\n",
    "    \"\"\"Node 1: Transcribe audio file with AssemblyAI\"\"\"\n",
    "    try:\n",
    "        print(f\"üéôÔ∏è Transcribing: {state['filename']}\")\n",
    "        \n",
    "        # Configure transcriber\n",
    "        transcriber = aai.Transcriber()\n",
    "        \n",
    "        # Transcribe the file\n",
    "        transcript = transcriber.transcribe(state['file_path'])\n",
    "        \n",
    "        if transcript.status == aai.TranscriptStatus.error:\n",
    "            return {\n",
    "                **state,\n",
    "                \"error\": f\"AssemblyAI error: {transcript.error}\",\n",
    "                \"status\": \"transcription_failed\"\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"transcript_text\": transcript.text,\n",
    "            \"status\": \"transcribed\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            \"error\": f\"Transcription error: {str(e)}\",\n",
    "            \"status\": \"transcription_failed\"\n",
    "        }\n",
    "\n",
    "def database_saver_node(state: AudioPipelineState) -> AudioPipelineState:\n",
    "    \"\"\"Node 2: Save transcript to database\"\"\"\n",
    "    try:\n",
    "        print(f\"üíæ Saving to database: {state['filename']}\")\n",
    "        \n",
    "        # Create conversation object\n",
    "        conversation = ConversationCreate(\n",
    "            title=f\"Audio: {state['filename']}\",\n",
    "            raw_text=state['transcript_text'],\n",
    "            source=\"transcribed\"\n",
    "        )\n",
    "        \n",
    "        # Save to database\n",
    "        conversation_id = db.create_conversation(conversation)\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"status\": \"completed\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            \"error\": f\"Database error: {str(e)}\",\n",
    "            \"status\": \"database_failed\"\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ LangGraph nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a76e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pain_extractor_node(state: AudioPipelineState) -> AudioPipelineState:\n",
    "    \"\"\"\n",
    "    LangGraph node: Extract structured insights from conversation transcript\n",
    "    \"\"\"\n",
    "    print(\"üß† Starting pain extraction...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract insights using OpenAI structured output\n",
    "        insights = extract_insights_from_transcript(state['transcript_text'])\n",
    "        \n",
    "        if insights:\n",
    "            print(f\"‚úÖ Extracted insights: {len(insights.primary_challenges)} primary challenges, {len(insights.speakers)} speakers\")\n",
    "            \n",
    "            return {\n",
    "                **state,\n",
    "                \"extracted_insights\": insights,\n",
    "                \"status\": \"insights_extracted\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                **state,\n",
    "                \"error\": \"Failed to extract insights from transcript\",\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pain extraction failed: {e}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"error\": f\"Pain extraction error: {str(e)}\",\n",
    "            \"status\": \"error\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c379c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangGraph pipeline compiled (4 nodes: transcribe ‚Üí save_to_db ‚Üí extract_insights ‚Üí creative_agent)\n"
     ]
    }
   ],
   "source": [
    "# Cell: Build Current Pipeline (4 Nodes) - FIXED\n",
    "def build_pipeline():\n",
    "    \"\"\"Build the current LangGraph workflow with transcription, save, insights, and creative agent\"\"\"\n",
    "    workflow = StateGraph(AudioPipelineState)\n",
    "    \n",
    "    # Add current nodes (use consistent naming - no spaces)\n",
    "    workflow.add_node(\"transcribe\", transcription_node)\n",
    "    workflow.add_node(\"save_to_db\", database_saver_node)  \n",
    "    workflow.add_node(\"extract_insights\", pain_extractor_node)\n",
    "    workflow.add_node(\"creative_agent\", creative_agent_node)  # ‚Üê Fixed: no space\n",
    "    \n",
    "    # Chain them together (use exact node names)\n",
    "    workflow.add_edge(\"transcribe\", \"save_to_db\")\n",
    "    workflow.add_edge(\"save_to_db\", \"extract_insights\")\n",
    "    workflow.add_edge(\"extract_insights\", \"creative_agent\")  # ‚Üê Fixed: consistent names\n",
    "    \n",
    "    workflow.set_entry_point(\"transcribe\")\n",
    "    workflow.set_finish_point(\"creative_agent\")  # ‚Üê Fixed: no space\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = build_pipeline()\n",
    "print(\"‚úÖ LangGraph pipeline compiled (4 nodes: transcribe ‚Üí save_to_db ‚Üí extract_insights ‚Üí creative_agent)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47a45a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Found 1 conversations to delete:\n",
      "  - ID 12: Audio: blog_record_(christian).wav\n",
      "‚úÖ All conversations deleted!\n",
      "‚úÖ Related blog ideas deleted!\n",
      "‚úÖ Processing status cleared!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Clean Conversations Table\n",
    "def clean_conversations_table():\n",
    "    \"\"\"Delete all records from conversations table\"\"\"\n",
    "    \n",
    "    # First show what will be deleted\n",
    "    conversations = db.get_all_conversations()\n",
    "    print(f\"üìä Found {len(conversations)} conversations to delete:\")\n",
    "    for conv in conversations[:5]:  # Show first 5\n",
    "        print(f\"  - ID {conv.id}: {conv.title}\")\n",
    "    if len(conversations) > 5:\n",
    "        print(f\"  ... and {len(conversations) - 5} more\")\n",
    "    \n",
    "    # Ask for confirmation\n",
    "    response = input(f\"\\n‚ùì Delete all {len(conversations)} conversations? (y/N): \")\n",
    "    \n",
    "    if response.lower() in ['y', 'yes']:\n",
    "        conn = db.get_connection()\n",
    "        try:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Delete all conversations (this will also delete related blog_post_ideas due to foreign key)\n",
    "            cursor.execute(\"DELETE FROM blog_post_ideas\")\n",
    "            cursor.execute(\"DELETE FROM processing_status\") \n",
    "            cursor.execute(\"DELETE FROM conversations\")\n",
    "            conn.commit()\n",
    "            \n",
    "            print(\"‚úÖ All conversations deleted!\")\n",
    "            print(\"‚úÖ Related blog ideas deleted!\")\n",
    "            print(\"‚úÖ Processing status cleared!\")\n",
    "            \n",
    "        finally:\n",
    "            conn.close()\n",
    "    else:\n",
    "        print(\"‚ùå Deletion cancelled\")\n",
    "\n",
    "# Run the cleaner\n",
    "clean_conversations_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cb00a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Add audio files to data/temp/ folder and rerun this cell\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Execute Batch Processing with Cleanup\n",
    "if files_available:\n",
    "    print(\"üéØ Starting batch processing...\")\n",
    "    \n",
    "    # Process all files\n",
    "    batch_results = process_audio_batch(audio_files, pipeline)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nüìä BATCH PROCESSING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Successfully processed: {len(batch_results['processed'])}\")\n",
    "    print(f\"‚ùå Failed: {len(batch_results['failed'])}\")\n",
    "    print(f\"üìÅ Total files: {batch_results['total']}\")\n",
    "    \n",
    "    # Show failed files\n",
    "    if batch_results['failed']:\n",
    "        print(f\"\\n‚ùå Failed files:\")\n",
    "        for file_path in batch_results['failed']:\n",
    "            print(f\"   - {file_path.name}\")\n",
    "    \n",
    "    # Cleanup successfully processed files\n",
    "    if batch_results['processed']:\n",
    "        confirm = input(f\"\\nüóëÔ∏è Delete {len(batch_results['processed'])} processed files? (y/N): \")\n",
    "        if confirm.lower() in ['y', 'yes']:\n",
    "            cleanup_processed_files(batch_results['processed'])\n",
    "        else:\n",
    "            print(\"üîß Files kept in temp folder for inspection\")\n",
    "    \n",
    "    print(\"\\nüéâ Batch processing complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"üí° Add audio files to data/temp/ folder and rerun this cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeaef0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Anthropic LLM initialized with Claude Haiku 4.5\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Setup Anthropic LLM for Insights Extraction (FIXED)\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "import json\n",
    "\n",
    "# Initialize Anthropic with correct model name\n",
    "anthropic_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not anthropic_key:\n",
    "    print(\"‚ö†Ô∏è  ANTHROPIC_API_KEY not found in .env file\")\n",
    "    print(\"Please add: ANTHROPIC_API_KEY=your_key_here\")\n",
    "else:\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-haiku-4-5\",  # ‚Üê Updated model name\n",
    "        api_key=anthropic_key,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    print(\"‚úÖ Anthropic LLM initialized with Claude Haiku 4.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b3cea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. PainExtractor Node Implementation\n",
    "\n",
    "\n",
    "import openai\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "# System prompt\n",
    "PAIN_EXTRACTOR_SYSTEM_PROMPT = \"\"\"\n",
    "You are a UX researcher and business analyst for BigKids Automation. Your job is listening to transcripts from interviews with users and potential clients. \n",
    "\n",
    "You pay special attention to problems that users have regarding how their company is automating, using web apps and AI to save time and move towards a more ethical and sovereign tech infrastructure.\n",
    "\n",
    "You will be given the transcript of an interview with a user or potential client.\n",
    "\n",
    "Your task is to extract structured information about:\n",
    "- Who is speaking and their role\n",
    "- What this person cares about (values, priorities)\n",
    "- Their main primary and secondary challenges\n",
    "- How they are solving problems today\n",
    "- Are there AI agents that can assist them?\n",
    "- Their underlying psychological needs (using frameworks like NVC - Non-Violent Communication)\n",
    "\n",
    "Focus on automation, web apps, AI, time-saving, ethical tech, and sovereign infrastructure themes.\n",
    "\n",
    "Be thorough but concise. \n",
    "\n",
    "IMPORTANT: Only extract information that is explicitly mentioned in the transcript. \n",
    "If information is not clearly stated, leave the field empty/null rather than guessing or inferring.\n",
    "Do not hallucinate or make assumptions about missing information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e1c7b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated extract_insights_from_transcript with speaker role fix\n"
     ]
    }
   ],
   "source": [
    "# Cell: Extract Insights Function - ROLE FIXED VERSION\n",
    "def extract_insights_from_transcript(transcript: str) -> ExtractedInsights:\n",
    "    \"\"\"Extract structured insights using Anthropic Claude - ROLE FIXED VERSION\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze this conversation transcript and extract structured insights:\n",
    "    \n",
    "    Transcript: {transcript}\n",
    "    \n",
    "    IMPORTANT: For speaker roles, use ONLY these exact values:\n",
    "    - \"client\" for the person being interviewed/consulted (CTO, CEO, Manager, business owner, etc.)\n",
    "    - \"interviewer\" for the person asking questions or conducting the interview\n",
    "    \n",
    "    Extract the following information in JSON format:\n",
    "    - speakers: List of people mentioned with name, role (client/interviewer only), company\n",
    "    - core_values: What they care about most  \n",
    "    - priorities: Current focus areas\n",
    "    - primary_challenges: Main problems they face with description, impact, urgency\n",
    "    - secondary_challenges: Secondary problems\n",
    "    - current_solutions: How they solve problems now with satisfaction level\n",
    "    - psychological_needs: Underlying needs with category, description, intensity\n",
    "    \n",
    "    Return ONLY valid JSON in this exact structure - no markdown, no code blocks:\n",
    "    {{\n",
    "        \"speakers\": [\n",
    "            {{\"name\": \"Manuel\", \"role\": \"client\", \"company\": \"Drone flytech\"}}\n",
    "        ],\n",
    "        \"core_values\": [\"efficiency\", \"transparency\"],\n",
    "        \"priorities\": [\"improving processes\"],\n",
    "        \"primary_challenges\": [\n",
    "            {{\n",
    "                \"description\": \"Tracking payment issues\",\n",
    "                \"impact\": \"Creates confusion in processes\", \n",
    "                \"urgency\": \"High\"\n",
    "            }}\n",
    "        ],\n",
    "        \"secondary_challenges\": [\n",
    "            {{\n",
    "                \"description\": \"Secondary challenge\",\n",
    "                \"impact\": \"Secondary impact\",\n",
    "                \"urgency\": \"Medium\"\n",
    "            }}\n",
    "        ],\n",
    "        \"current_solutions\": [\n",
    "            {{\n",
    "                \"solution\": \"Current approach\",\n",
    "                \"satisfaction_level\": \"Neutral\",\n",
    "                \"limitations\": [\"limitation1\", \"limitation2\"]\n",
    "            }}\n",
    "        ],\n",
    "        \"psychological_needs\": [\n",
    "            {{\n",
    "                \"need_category\": \"security\",\n",
    "                \"description\": \"Need for confidence\",\n",
    "                \"intensity\": \"High\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "    \n",
    "    Remember: \n",
    "    - Use \"client\" for Manuel (even though he's CTO)\n",
    "    - Use \"interviewer\" for the person asking questions\n",
    "    - Use exact urgency values: \"Low\", \"Medium\", \"High\"\n",
    "    - Use exact satisfaction levels: \"Very Satisfied\", \"Satisfied\", \"Neutral\", \"Unsatisfied\", \"Very Unsatisfied\"\n",
    "    - Use exact intensity values: \"Low\", \"Medium\", \"High\"\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Use the Claude LLM you already set up\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        print(f\"üìù Raw response length: {len(response.content)} chars\")\n",
    "        print(f\"üìù Response starts with: {response.content[:50]}...\")\n",
    "        \n",
    "        # Clean markdown code blocks\n",
    "        content = response.content.strip()\n",
    "        if content.startswith('```json'):\n",
    "            print(\"üîß Removing JSON markdown blocks...\")\n",
    "            content = content.replace('```json', '').replace('```', '').strip()\n",
    "            print(f\"üîß Cleaned content starts with: {content[:50]}...\")\n",
    "        \n",
    "        # Parse the cleaned JSON response\n",
    "        insights_data = json.loads(content)\n",
    "        \n",
    "        # Convert to Pydantic model\n",
    "        result = ExtractedInsights(**insights_data)\n",
    "        print(f\"‚úÖ Successfully extracted insights with correct speaker roles!\")\n",
    "        return result\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON parsing error: {e}\")\n",
    "        print(f\"üìù Raw response: {response.content[:500]}...\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in LLM call: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"‚úÖ Updated extract_insights_from_transcript with speaker role fix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6a3dc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No conversation found to test with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell: Test Fixed Pain Extraction\n",
    "def test_pain_extraction_fix():\n",
    "    \"\"\"Test the fixed pain extraction with your transcript\"\"\"\n",
    "    \n",
    "    # Get the conversation that just failed\n",
    "    conversations = db.get_all_conversations()\n",
    "    latest_conversation = conversations[0] if conversations else None\n",
    "    \n",
    "    if latest_conversation and latest_conversation.raw_text:\n",
    "        print(\"üß™ Testing fixed pain extraction...\")\n",
    "        print(f\"üìù Using conversation: {latest_conversation.title}\")\n",
    "        \n",
    "        try:\n",
    "            # Test the fixed extraction\n",
    "            insights = extract_insights_from_transcript(latest_conversation.raw_text)\n",
    "            print(f\"‚úÖ Success! Extracted {len(insights.primary_challenges)} challenges\")\n",
    "            print(f\"üë• Found {len(insights.speakers)} speakers\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Still failing: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"‚ùå No conversation found to test with\")\n",
    "        return False\n",
    "\n",
    "# Test the fix\n",
    "test_pain_extraction_fix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3626c104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 0 audio files in temp:\n"
     ]
    }
   ],
   "source": [
    "# Cell: Audio File Finder Function\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def find_audio_files_in_temp():\n",
    "    \"\"\"Find all audio files in temp folder\"\"\"\n",
    "    temp_folder = Path(\"../data/temp\")  # Adjust path based on notebook location\n",
    "    \n",
    "    if not temp_folder.exists():\n",
    "        print(f\"‚ùå Temp folder not found: {temp_folder}\")\n",
    "        return []\n",
    "    \n",
    "    # Find audio files\n",
    "    audio_extensions = ['*.wav', '*.mp3', '*.m4a']\n",
    "    audio_files = []\n",
    "    \n",
    "    for ext in audio_extensions:\n",
    "        files = list(temp_folder.glob(ext))\n",
    "        audio_files.extend(files)\n",
    "    \n",
    "    return sorted(audio_files)\n",
    "\n",
    "# Test the function\n",
    "audio_files = find_audio_files_in_temp()\n",
    "print(f\"üìÅ Found {len(audio_files)} audio files in temp:\")\n",
    "for file in audio_files:\n",
    "    size_mb = file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"   {file.name} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3eb0478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ EXECUTING COMPLETE 4-NODE PIPELINE TEST...\n",
      "This will test: Audio ‚Üí Transcribe ‚Üí Save ‚Üí Insights ‚Üí Blog Ideas\n",
      "------------------------------------------------------------\n",
      "üöÄ TESTING COMPLETE 4-NODE PIPELINE\n",
      "============================================================\n",
      "Flow: Audio ‚Üí Transcribe ‚Üí Save to DB ‚Üí Extract Insights ‚Üí Generate Ideas\n",
      "============================================================\n",
      "üìÅ Found 1 audio files\n",
      "üéØ Testing with: blog_record_(manuelillo_cto).wav\n",
      "üìä File size: 19758.0 KB\n",
      "\n",
      "üé¨ STARTING COMPLETE PIPELINE EXECUTION...\n",
      "============================================================\n",
      "‚è≥ Running pipeline.invoke()...\n",
      "üéôÔ∏è Transcribing: blog_record_(manuelillo_cto).wav\n",
      "üíæ Saving to database: blog_record_(manuelillo_cto).wav\n",
      "üß† Starting pain extraction...\n",
      "üìù Raw response length: 2538 chars\n",
      "üìù Response starts with: ```json\n",
      "{\n",
      "    \"speakers\": [\n",
      "        {\"name\": \"Manu...\n",
      "üîß Removing JSON markdown blocks...\n",
      "üîß Cleaned content starts with: {\n",
      "    \"speakers\": [\n",
      "        {\"name\": \"Manuel\", \"ro...\n",
      "‚úÖ Successfully extracted insights with correct speaker roles!\n",
      "‚úÖ Extracted insights: 3 primary challenges, 1 speakers\n",
      "üé® Starting creative blog idea generation...\n",
      "‚úÖ Loaded company strategy (6555 chars)\n",
      "‚úÖ Loaded SEO strategy (5566 chars)\n",
      "üìù Raw response length: 4060 chars\n",
      "üìù Response starts with: ```json\n",
      "[\n",
      "    {\n",
      "        \"title\": \"From Chaos to Cl...\n",
      "üîß Removing markdown code blocks...\n",
      "üîß Cleaned content starts with: [\n",
      "    {\n",
      "        \"title\": \"From Chaos to Clarity: H...\n",
      "‚úÖ Creative agent successfully parsed 5 blog ideas\n",
      "üéâ Generated 5 valid blog ideas\n",
      "\n",
      "üìä COMPLETE PIPELINE RESULTS:\n",
      "============================================================\n",
      "üéØ Final Status: raw_ideas_generated\n",
      "\n",
      "üìã STAGE RESULTS:\n",
      "   üéôÔ∏è  Transcription: ‚úÖ\n",
      "   üíæ Database Save: ‚úÖ\n",
      "   üß† Insights Extraction: ‚úÖ\n",
      "   üé® Blog Ideas Generation: ‚úÖ\n",
      "\n",
      "üéâ COMPLETE SUCCESS! End-to-end pipeline worked!\n",
      "======================================================================\n",
      "üìù Conversation ID: 14\n",
      "üìä Transcript Length: 1688 characters\n",
      "üß† Extracted Insights:\n",
      "   üë• Speakers: 1\n",
      "   üî• Primary Challenges: 3\n",
      "   üßò Psychological Needs: 4\n",
      "   üíé Core Values: 5\n",
      "\n",
      "üé® Generated Blog Ideas (5):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üí° IDEA 1:\n",
      "   üìù Title: From Chaos to Clarity: How Automation Fixes Invoice Tracking Without Losing Control\n",
      "   üìÑ Description: A practical guide exploring how SMEs can implement AI-powered invoice tracking systems that provide ...\n",
      "   üéØ Target Audience: Finance managers and business owners at SMEs struggling with manual invoice tracking and payment visibility\n",
      "   üìà Business Value: Directly addresses high-urgency pain points (tracking, visibility, confidence) while positioning Big Kids as a trusted guide for financial automation\n",
      "\n",
      "üí° IDEA 2:\n",
      "   üìù Title: The Hidden Cost of 'Good Enough' Invoicing Software: When to Automate Instead of Tolerate\n",
      "   üìÑ Description: An honest exploration of why off-the-shelf invoicing solutions often fail growing businesses, and ho...\n",
      "   üéØ Target Audience: Founders and finance leads at scaling SMEs currently dissatisfied with existing SaaS solutions\n",
      "   üìà Business Value: Speaks directly to Manuel's MoneyOak dissatisfaction; positions custom automation as the intelligent alternative\n",
      "\n",
      "üí° IDEA 3:\n",
      "   üìù Title: Building Financial Transparency: How Automation Scales Your Invoicing Without Scaling Your Headaches\n",
      "   üìÑ Description: A deep dive into how AI and automation handle the complexity of multi-client invoicing as businesses...\n",
      "   üéØ Target Audience: Operations managers and finance teams at rapidly growing service-based businesses\n",
      "   üìà Business Value: Addresses urgency around managing complexity; attracts growth-stage companies before they become too large to help\n",
      "\n",
      "üí° IDEA 4:\n",
      "   üìù Title: Trust the System: How Transparent Automation Reduces Financial Anxiety in Growing Teams\n",
      "   üìÑ Description: Explores the psychological and operational benefits of automating financial processes‚Äîmoving from an...\n",
      "   üéØ Target Audience: Business owners and finance leaders seeking to reduce operational stress and build team confidence\n",
      "   üìà Business Value: Aligns with Big Kids' values (care, work smart not hard) while addressing Manuel's core psychological needs (confidence, peace of mind, trust)\n",
      "\n",
      "üí° IDEA 5:\n",
      "   üìù Title: Custom Invoice Automation for Service Businesses: When Off-the-Shelf Tools Aren't Enough\n",
      "   üìÑ Description: A practical playbook for building custom invoicing solutions tailored to unique business models. Cov...\n",
      "   üéØ Target Audience: Service-based SME owners with complex or non-standard invoicing needs\n",
      "   üìà Business Value: Directly supports SEO strategy around GenAI business value; generates qualified leads ready for custom automation conversations\n",
      "======================================================================\n",
      "üéâ COMPLETE 4-NODE PIPELINE: SUCCESS!\n",
      "‚úÖ System is working end-to-end!\n",
      "üöÄ Ready to build Node 5 (Analyst Agent)\n",
      "\n",
      "üìã FINAL TEST SUMMARY:\n",
      "   Test Status: SUCCESS\n",
      "   Pipeline Status: raw_ideas_generated\n",
      "   Blog Ideas Generated: 5\n"
     ]
    }
   ],
   "source": [
    "# Cell: Complete 4-Node Pipeline Test\n",
    "def test_complete_4_node_pipeline():\n",
    "    \"\"\"Test the complete pipeline: Audio ‚Üí Transcribe ‚Üí Save ‚Üí Insights ‚Üí Blog Ideas\"\"\"\n",
    "    \n",
    "    print(\"üöÄ TESTING COMPLETE 4-NODE PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Flow: Audio ‚Üí Transcribe ‚Üí Save to DB ‚Üí Extract Insights ‚Üí Generate Ideas\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if pipeline is built\n",
    "    if 'pipeline' not in globals():\n",
    "        print(\"‚ùå Pipeline not found!\")\n",
    "        print(\"üí° Please run the build_pipeline() cell first\")\n",
    "        return None\n",
    "    \n",
    "    # Find audio files in temp\n",
    "    audio_files = find_audio_files_in_temp()\n",
    "    \n",
    "    if not audio_files:\n",
    "        print(\"‚ùå No audio files found in data/temp/ folder\")\n",
    "        print(\"\\nüí° SOLUTIONS:\")\n",
    "        print(\"   1. Add a .wav file manually to data/temp/\")\n",
    "        print(\"   2. Disable cleanup in file_monitor.py and upload new file\")\n",
    "        print(\"   3. Copy an existing audio file:\")\n",
    "        print(\"      cp /path/to/audio.wav data/temp/test_file.wav\")\n",
    "        return None\n",
    "    \n",
    "    # Use the first audio file\n",
    "    test_file = audio_files[0]\n",
    "    print(f\"üìÅ Found {len(audio_files)} audio files\")\n",
    "    print(f\"üéØ Testing with: {test_file.name}\")\n",
    "    print(f\"üìä File size: {test_file.stat().st_size / 1024:.1f} KB\")\n",
    "    \n",
    "    # Create initial state for complete 4-node pipeline\n",
    "    initial_state = {\n",
    "        \"file_path\": str(test_file),\n",
    "        \"filename\": test_file.name,\n",
    "        \"transcript_text\": None,           # Will be filled by Node 1\n",
    "        \"conversation_id\": None,           # Will be filled by Node 2  \n",
    "        \"extracted_insights\": None,        # Will be filled by Node 3\n",
    "        \"raw_blog_ideas\": None,            # Will be filled by Node 4\n",
    "        \"scored_blog_ideas\": None,         # For future Node 5\n",
    "        \"saved_idea_ids\": None,            # For future Node 6\n",
    "        \"error\": None,\n",
    "        \"status\": \"processing\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüé¨ STARTING COMPLETE PIPELINE EXECUTION...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Execute the complete 4-node pipeline\n",
    "        print(\"‚è≥ Running pipeline.invoke()...\")\n",
    "        final_state = pipeline.invoke(initial_state)\n",
    "        \n",
    "        print(f\"\\nüìä COMPLETE PIPELINE RESULTS:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Check final status\n",
    "        final_status = final_state.get('status', 'unknown')\n",
    "        print(f\"üéØ Final Status: {final_status}\")\n",
    "        \n",
    "        # Check each stage\n",
    "        print(f\"\\nüìã STAGE RESULTS:\")\n",
    "        print(f\"   üéôÔ∏è  Transcription: {'‚úÖ' if final_state.get('transcript_text') else '‚ùå'}\")\n",
    "        print(f\"   üíæ Database Save: {'‚úÖ' if final_state.get('conversation_id') else '‚ùå'}\")\n",
    "        print(f\"   üß† Insights Extraction: {'‚úÖ' if final_state.get('extracted_insights') else '‚ùå'}\")\n",
    "        print(f\"   üé® Blog Ideas Generation: {'‚úÖ' if final_state.get('raw_blog_ideas') else '‚ùå'}\")\n",
    "        \n",
    "        # Show detailed results if successful\n",
    "        if final_state.get('raw_blog_ideas'):\n",
    "            ideas = final_state['raw_blog_ideas']\n",
    "            insights = final_state.get('extracted_insights')\n",
    "            \n",
    "            print(f\"\\nüéâ COMPLETE SUCCESS! End-to-end pipeline worked!\")\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"üìù Conversation ID: {final_state.get('conversation_id')}\")\n",
    "            print(f\"üìä Transcript Length: {len(final_state.get('transcript_text', ''))} characters\")\n",
    "            \n",
    "            if insights:\n",
    "                print(f\"üß† Extracted Insights:\")\n",
    "                print(f\"   üë• Speakers: {len(insights.speakers)}\")\n",
    "                print(f\"   üî• Primary Challenges: {len(insights.primary_challenges)}\")\n",
    "                print(f\"   üßò Psychological Needs: {len(insights.psychological_needs)}\")\n",
    "                print(f\"   üíé Core Values: {len(insights.core_values)}\")\n",
    "            \n",
    "            print(f\"\\nüé® Generated Blog Ideas ({len(ideas)}):\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for i, idea in enumerate(ideas, 1):\n",
    "                print(f\"\\nüí° IDEA {i}:\")\n",
    "                # Handle both dict and Pydantic object formats\n",
    "                if hasattr(idea, 'title'):\n",
    "                    # Pydantic object\n",
    "                    print(f\"   üìù Title: {idea.title}\")\n",
    "                    print(f\"   üìÑ Description: {idea.description[:100]}...\")\n",
    "                    print(f\"   üéØ Target Audience: {idea.target_audience}\")\n",
    "                    print(f\"   üìà Business Value: {idea.business_value}\")\n",
    "                else:\n",
    "                    # Dictionary\n",
    "                    print(f\"   üìù Title: {idea.get('title', 'No title')}\")\n",
    "                    print(f\"   üìÑ Description: {idea.get('description', 'No description')[:100]}...\")\n",
    "                    print(f\"   üéØ Target Audience: {idea.get('target_audience', 'Unknown')}\")\n",
    "                    print(f\"   üìà Business Value: {idea.get('business_value', 'Unknown')}\")\n",
    "            \n",
    "            print(\"=\" * 70)\n",
    "            print(\"üéâ COMPLETE 4-NODE PIPELINE: SUCCESS!\")\n",
    "            print(\"‚úÖ System is working end-to-end!\")\n",
    "            print(\"üöÄ Ready to build Node 5 (Analyst Agent)\")\n",
    "            \n",
    "            return final_state\n",
    "            \n",
    "        else:\n",
    "            # Pipeline failed somewhere\n",
    "            print(f\"\\n‚ùå PIPELINE INCOMPLETE\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            error_msg = final_state.get('error', 'No specific error message')\n",
    "            print(f\"‚ùå Error: {error_msg}\")\n",
    "            print(f\"üîç Status: {final_status}\")\n",
    "            \n",
    "            # Debug info\n",
    "            print(f\"\\nüîç DEBUG INFO:\")\n",
    "            print(f\"   Transcript exists: {bool(final_state.get('transcript_text'))}\")\n",
    "            if final_state.get('transcript_text'):\n",
    "                print(f\"   Transcript preview: {final_state['transcript_text'][:100]}...\")\n",
    "            print(f\"   Insights exist: {bool(final_state.get('extracted_insights'))}\")\n",
    "            print(f\"   Ideas exist: {bool(final_state.get('raw_blog_ideas'))}\")\n",
    "            \n",
    "            return final_state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå COMPLETE PIPELINE EXECUTION FAILED!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üí• Exception: {str(e)}\")\n",
    "        \n",
    "        # Show full traceback for debugging\n",
    "        import traceback\n",
    "        print(f\"\\nüîç FULL ERROR TRACEBACK:\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(f\"\\nüí° DEBUGGING TIPS:\")\n",
    "        print(\"   1. Check if all 4 nodes are properly defined\")\n",
    "        print(\"   2. Verify AssemblyAI API key is working\")\n",
    "        print(\"   3. Check Anthropic API key is working\")\n",
    "        print(\"   4. Ensure audio file is not corrupted\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Execute the complete pipeline test\n",
    "print(\"üß™ EXECUTING COMPLETE 4-NODE PIPELINE TEST...\")\n",
    "print(\"This will test: Audio ‚Üí Transcribe ‚Üí Save ‚Üí Insights ‚Üí Blog Ideas\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "complete_test_result = test_complete_4_node_pipeline()\n",
    "\n",
    "# Show final summary\n",
    "if complete_test_result:\n",
    "    print(f\"\\nüìã FINAL TEST SUMMARY:\")\n",
    "    print(f\"   Test Status: {'SUCCESS' if complete_test_result.get('raw_blog_ideas') else 'PARTIAL/FAILED'}\")\n",
    "    print(f\"   Pipeline Status: {complete_test_result.get('status', 'unknown')}\")\n",
    "    print(f\"   Blog Ideas Generated: {len(complete_test_result.get('raw_blog_ideas', []))}\")\n",
    "else:\n",
    "    print(f\"\\nüìã FINAL TEST SUMMARY:\")\n",
    "    print(f\"   Test Status: FAILED\")\n",
    "    print(f\"   Pipeline could not complete execution\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
